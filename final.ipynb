{"cells": [{"cell_type": "markdown", "id": "850356b3", "metadata": {"id": "850356b3"}, "source": ["# Laboratorio 4\n", "\n", "Sean bienvenidos de nuevo al laboratorio 4 de Deep Learning y Sistemas Inteligentes. As\u00ed como en los laboratorios pasados, espero que esta ejercitaci\u00f3n les sirva para consolidar sus conocimientos en el tema de Encoder-Decoder y AutoEnconders.\n", "\n", "Para este laboratorio estaremos usando una herramienta para Jupyter Notebooks que facilitar\u00e1 la calificaci\u00f3n, no solo asegur\u00e1ndo que ustedes tengan una nota pronto sino tambi\u00e9n mostrandoles su nota final al terminar el laboratorio.\n", "\n", "Espero que esta vez si se muestren los *marks*. De nuevo me discupo si algo no sale bien, seguiremos mejorando conforme vayamos iterando. Siempre pido su comprensi\u00f3n y colaboraci\u00f3n si algo no funciona como deber\u00eda.\n", "\n", "Al igual que en el laboratorio pasado, estaremos usando la librer\u00eda de Dr John Williamson et al de la University of Glasgow, adem\u00e1s de ciertas piezas de c\u00f3digo de Dr Bjorn Jensen de su curso de Introduction to Data Science and System de la University of Glasgow para la visualizaci\u00f3n de sus calificaciones.\n", "\n", "**NOTA:** Ahora tambien hay una tercera dependecia que se necesita instalar. Ver la celda de abajo por favor\n", "\n", "<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\">\n", "</script>"]}, {"cell_type": "code", "execution_count": 1, "id": "49183e54", "metadata": {"ExecuteTime": {"end_time": "2023-08-06T06:29:54.419993Z", "start_time": "2023-08-06T06:29:54.409473Z"}, "colab": {"base_uri": "https://localhost:8080/"}, "id": "49183e54", "outputId": "4d1398ad-845f-427d-d7f0-ad9c55ba3abe"}, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["Collecting https://github.com/johnhw/jhwutils/zipball/master\n", "  Downloading https://github.com/johnhw/jhwutils/zipball/master\n", "\u001b[2K     \u001b[32m\\\u001b[0m \u001b[32m119.1 kB\u001b[0m \u001b[31m641.6 kB/s\u001b[0m \u001b[33m0:00:00\u001b[0m\n", "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n", "Building wheels for collected packages: jhwutils\n", "  Building wheel for jhwutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n", "  Created wheel for jhwutils: filename=jhwutils-1.3-py3-none-any.whl size=41854 sha256=7dce59af7d598da81b5fd423a23900c319ec7ae711ea011030487401dc57dbc4\n", "  Stored in directory: /tmp/pip-ephem-wheel-cache-2r4qxr9m/wheels/a8/e7/e3/9542f8e4159ba644c6acd9f78babbe8489bb72667fb02ac54d\n", "Successfully built jhwutils\n", "Installing collected packages: jhwutils\n", "Successfully installed jhwutils-1.3\n", "Collecting https://github.com/AlbertS789/lautils/zipball/master\n", "  Downloading https://github.com/AlbertS789/lautils/zipball/master\n", "\u001b[2K     \u001b[32m-\u001b[0m \u001b[32m4.2 kB\u001b[0m \u001b[31m?\u001b[0m \u001b[33m0:00:00\u001b[0m\n", "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n", "Building wheels for collected packages: lautils\n", "  Building wheel for lautils (setup.py) ... \u001b[?25l\u001b[?25hdone\n", "  Created wheel for lautils: filename=lautils-1.0-py3-none-any.whl size=2826 sha256=05c6df8735869e376fa6a296edba5414a11638386c3c02b0c023443cf6c56fde\n", "  Stored in directory: /tmp/pip-ephem-wheel-cache-vbmj2mai/wheels/1a/50/ba/b3ceb937949f5894a896b68af5b5fdb598e50244141063e4db\n", "Successfully built lautils\n", "Installing collected packages: lautils\n", "Successfully installed lautils-1.0\n"]}], "source": ["# Una vez instalada la librer\u00eda por favor, recuerden volverla a comentar.\n", "!pip install -U --force-reinstall --no-cache https://github.com/johnhw/jhwutils/zipball/master\n", "#!pip install scikit-image\n", "!pip install -U --force-reinstall --no-cache https://github.com/AlbertS789/lautils/zipball/master"]}, {"cell_type": "code", "execution_count": 31, "id": "1c2378f6", "metadata": {"ExecuteTime": {"end_time": "2023-08-07T22:24:37.953793Z", "start_time": "2023-08-07T22:24:34.644956Z"}, "id": "1c2378f6"}, "outputs": [], "source": ["import numpy as np\n", "import copy\n", "import matplotlib.pyplot as plt\n", "import scipy\n", "from PIL import Image\n", "import os\n", "from collections import defaultdict\n", "\n", "#from IPython import display\n", "#from base64 import b64decode\n", "\n", "\n", "# Other imports\n", "from unittest.mock import patch\n", "from uuid import getnode as get_mac\n", "\n", "from jhwutils.checkarr import array_hash, check_hash, check_scalar, check_string, array_hash, _check_scalar\n", "import jhwutils.image_audio as ia\n", "import jhwutils.tick as tick\n", "from lautils.gradeutils import new_representation, hex_to_float, compare_numbers, compare_lists_by_percentage, calculate_coincidences_percentage\n", "\n", "###\n", "tick.reset_marks()\n", "\n", "%matplotlib inline"]}, {"cell_type": "code", "execution_count": 32, "id": "872e6c48", "metadata": {"ExecuteTime": {"end_time": "2023-08-06T06:29:55.567829Z", "start_time": "2023-08-06T06:29:55.560965Z"}, "deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "29e52b805cfebe42903d0379a3f485da", "grade": false, "grade_id": "cell-95b81aaa3e57306b", "locked": true, "schema_version": 3, "solution": false, "task": false}, "id": "872e6c48"}, "outputs": [], "source": ["# Seeds\n", "seed_ = 2023\n", "np.random.seed(seed_)"]}, {"cell_type": "code", "execution_count": null, "id": "d2e571e0", "metadata": {"ExecuteTime": {"end_time": "2023-08-06T06:29:55.581630Z", "start_time": "2023-08-06T06:29:55.567829Z"}, "deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "3aa8961ba46ffd91e0ae666686e967e7", "grade": true, "grade_id": "cell-b2ae10e4b3198bb2", "locked": true, "points": 0, "schema_version": 3, "solution": false, "task": false}, "id": "d2e571e0"}, "outputs": [], "source": ["# Celda escondida para utlidades necesarias, por favor NO edite esta celda\n"]}, {"cell_type": "markdown", "id": "1a97c050", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "ea27899c011d00466ba84d10df3c8450", "grade": false, "grade_id": "cell-37707c73cc6055e5", "locked": true, "schema_version": 3, "solution": false, "task": false}, "id": "1a97c050"}, "source": ["###### Informaci\u00f3n del estudiante en dos variables\n", "\n", "* carne_1 : un string con su carne (e.g. \"12281\"), debe ser de al menos 5 caracteres.\n", "* firma_mecanografiada_1: un string con su nombre (e.g. \"Albero Suriano\") que se usar\u00e1 para la declaracion que este trabajo es propio (es decir, no hay plagio)\n", "* carne_2 : un string con su carne (e.g. \"12281\"), debe ser de al menos 5 caracteres.\n", "* firma_mecanografiada_2: un string con su nombre (e.g. \"Albero Suriano\") que se usar\u00e1 para la declaracion que este trabajo es propio (es decir, no hay plagio)"]}, {"cell_type": "code", "execution_count": null, "id": "e766e448", "metadata": {"ExecuteTime": {"end_time": "2023-08-06T06:29:55.588643Z", "start_time": "2023-08-06T06:29:55.581630Z"}, "deletable": false, "nbgrader": {"cell_type": "code", "checksum": "e7c7bd38d70a53f41a59434e097ebf75", "grade": false, "grade_id": "cell-887917342d3eaa54", "locked": false, "schema_version": 3, "solution": true, "task": false}, "id": "e766e448"}, "outputs": [], "source": ["carne_1 = \"22933\"\n", "firma_mecanografiada_1 = \"Daniel Alfredo Rayo Roldan\"\n", "carne_2 = \"22386\"\n", "firma_mecanografiada_2 = \"Flavio Andr\u00e9 Gal\u00e1n Donis\"\n", "# YOUR CODE HERE\n", "# raise NotImplementedError()"]}, {"cell_type": "code", "execution_count": null, "id": "4d41a5c0", "metadata": {"ExecuteTime": {"end_time": "2023-08-06T06:29:55.602639Z", "start_time": "2023-08-06T06:29:55.588643Z"}, "deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "6069d482a40ebc901473d44861baeb63", "grade": true, "grade_id": "cell-4aa33cdbf61b184d", "locked": true, "points": 0, "schema_version": 3, "solution": false, "task": false}, "colab": {"base_uri": "https://localhost:8080/", "height": 128}, "id": "4d41a5c0", "outputId": "f95e380a-12b5-4caa-9531-897aba765f29"}, "outputs": [{"output_type": "display_data", "data": {"text/plain": ["<IPython.core.display.HTML object>"], "text/html": ["\n", "        <div class=\"alert alert-box alert-success\">\n", "        <h1> <!--{id:\"CORRECTMARK\", marks:\"0\"}--> \n", "         \u2713 [0 marks] \n", "         </h1> </div>"]}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["<IPython.core.display.HTML object>"], "text/html": ["\n", "        <div class=\"alert alert-box alert-success\">\n", "        <h1> <!--{id:\"CORRECTMARK\", marks:\"0\"}--> \n", "         \u2713 [0 marks] \n", "         </h1> </div>"]}, "metadata": {}}], "source": ["# Deberia poder ver dos checkmarks verdes [0 marks], que indican que su informaci\u00f3n b\u00e1sica est\u00e1 OK\n", "\n", "with tick.marks(0):\n", "    assert(len(carne_1)>=5 and len(carne_2)>=5)\n", "\n", "with tick.marks(0):\n", "    assert(len(firma_mecanografiada_1)>0 and len(firma_mecanografiada_2)>0)"]}, {"cell_type": "markdown", "id": "96c98973", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "1c296a7f61dad354a3388f85563084bc", "grade": false, "grade_id": "cell-d37c69d4d3712b18", "locked": true, "schema_version": 3, "solution": false, "task": false}, "id": "96c98973"}, "source": ["## Parte 1 - Word2Vec\n", "\n", "**Cr\u00e9ditos:** La primera parte de este laboratorio est\u00e1 tomado y basado en uno de los post de Musashi (Jacobs-) Harukawa\n", "\n", "La eficacia de las t\u00e9cnicas de embedding est\u00e1 directamente relacionada con los desaf\u00edos iniciales que motivaron los enfoques de texto como datos. Al convertir el lenguaje natural en representaciones num\u00e9ricas, los m\u00e9todos de incrustaci\u00f3n abren oportunidades para aplicar varias herramientas cuantitativas a fuentes de datos previamente sin explotar.\n", "\n", "En t\u00e9rminos generales, word embedding representa cada palabra en un conjunto dado de textos (corpus) como vectores en un espacio k-dimensional (donde k es elegido por el investigador; m\u00e1s detalles sobre esto m\u00e1s adelante). Estos vectores contienen informaci\u00f3n valiosa sobre las relaciones de las palabras y su contexto, sirviendo como herramientas esenciales para las tareas posteriores de modelado del lenguaje.\n", "\n", "Entonces, es entendible que se pregunten\n", "\n", "* \u00bfC\u00f3mo funciona este proceso de incrustaci\u00f3n?\n", "* \u00bfCu\u00e1l es la raz\u00f3n subyacente de su \u00e9xito?\n", "* \u00bfC\u00f3mo podemos determinar su eficacia?\n", "\n", "Para poder responder las primeras dos preguntas, vamos a implementar este modelo usando PyTorch. Noten que el state-of-the-art ya no solo se usa Word2Vec, como BERT (Bidirectional Encoder Representations from Transformers). Pero siempre es un buen ejercicio entender estos algoritmos.\n", "\n", "### Paso 1 - DataLoader\n", "\n", "Como en laboratorios, lo primero que necesitamos es definir un DataLoader. Para esta primera parte estaremos usando el dataset llamado \"tweets_hate_speech_detection\" de HugginFace.\n", "\n", "Para esto necesitamos una funci\u00f3n que separe los textos en listas de tokens. El preprocesamiento para cuando se trabaja con textos debe ser un poco m\u00e1s exhaustivo de lo que haremos en este laboratorio, pero para fines del mismo solamente haremos:\n", "\n", "1- Pasar a minusculas\n", "\n", "2- Quitar todos los simbolos diferentes de a-z@#\n", "\n", "3- Separar en espacios\n", "\n", "4- Quitar \"stopword\" y tokens vac\u00edos\n", "\n", "5- Aplicar snowball stemmer al resto (snowball? s\u00ed, refieran a la nota de abajo para la explicaci\u00f3n r\u00e1pida)\n", "\n", "Para esto nos apoyaremos en el paquete de natural language processing toolkit o nltk para los cuates. Entonces, recuerden instalarlo por favor \"pip install nltk\"\n", "\n", "**Snowball Stemmer** es un modulo en la librer\u00eda NLTK que implementa la t\u00e9cnica de stemming. \u00bfStemming?\n", "Stemming es una t\u00e9cnica utilizada para extraer la forma base de las palabras mediante la eliminaci\u00f3n de los (pre-post)fijos de ellos. Imaginen que cortan la ramas de un \u00e1rbol hasta los tallos. Por ejemplo, la ra\u00edz de las palabras comiendo, come, comido es comer. Refieran a este [link](https://www.tutorialspoint.com/natural_language_toolkit/natural_language_toolkit_stemming_lemmatization.htm) para m\u00e1s informaci\u00f3n"]}, {"cell_type": "code", "execution_count": null, "id": "27b5c601", "metadata": {"ExecuteTime": {"end_time": "2023-08-06T06:29:58.840015Z", "start_time": "2023-08-06T06:29:55.602639Z"}, "colab": {"base_uri": "https://localhost:8080/", "height": 301, "referenced_widgets": ["822b06c6edad46aa97086a8eb24b529f", "46a814a3dff24304a9827996b1b17ba6", "73ad7418592b4d4dafa7cff6e0479e48", "a2bd24080f694dab97584227a33bd15d", "ef06f016e6224f6bb34242a569b978f8", "85a3ec21e2dc4a7bb7e6f2b624db4f35", "b06c521beeb0484ab26f1f6777e3fd39", "b2ccc571b1ae4a22aa2f07d91ffc1982", "ba311895ae2f441888dcb34b500d1d4f", "311c4805d59341c89277f1ea5e8030a4", "71c8413d3f2b4546ab2ca5e2ab17fc84", "e2a73548943447fa8b1c7d6a2cc9f0dd", "5673bc25e0ab408b9a6ceb9e3931bdde", "b9ba3986526f404db2e9a6edc7947367", "dfa4a69b1a5a4c01939e351937dcae80", "6ae8a6a99d9441829d1b9a196d772ee6", "423204beaa984aa6ad0a6396af1319f7", "5e38add836454ae99d33098d85712b25", "e2d48af9b6f54847ad89f8f5d54f2077", "46bb8aff6cae4a3094db4f92401ed5ec", "c22d1ec5f2684b05aaac3478b8497d31", "236eb25c1db74e7782c56d95f41efdf8", "05e9876fc12f4c86adaf0b16ea960cf1", "1487fc0512ff4a758bc32d6e03ebdb19", "b5b55d83108d40c79f3913e246710082", "be59aa2898634dea833140dd1ae121e5", "f5189231ce824e00a1e1db51d76d83aa", "cbaf1997f70c495f92e6aa7e7d41e73e", "cd07ec76077c491ab156b347424a80e5", "d28b092b5205459d8db0e563eecb103e", "1b369b2b232d4253949a38c3ebda2a0f", "488f109bd9944239ab84027134f83389", "58ffb4303adc4db4a9673c6313af9789", "258542da13d74058954ff9113cc68e4b", "1585059b6e3d4ca98bd783bfd753d81c", "2b81483103b14cfd96670f175373a23e", "dc4b24c50a3f41cbb4ba3ac5854e88db", "8ddf56d0aaf642679e466a0521560101", "36f43eeef4664352aaa20bdcf9ddb8dd", "0e1693cf99454998a82a4c3de10be455", "533820de87da4f81a9bf050b9ff9c7df", "baff436ea7ea4d4f9fe1d8eb03c907ab", "16b4008c6a98458b9a8e86b4b28ce07e", "1b54241282a04710af20970b7511156d", "84582b5aa04e4b079f829fc475827b97", "e14e440001af4ce5b8509d643eb05e2e", "b6040dbaef1a447794f669966e220084", "a30db461f1ad4977b9f21b6cae37cd06", "eb20fc64ccde43d4b3ba0037f327c12c", "6729b757046540ec815aaf8f74ea098a", "7f5b77c9d0854c5fa186b983ab7bf1ca", "4526d14c2305464197efcff8754665de", "c52a5453a14b427b92cc173865e97d9d", "a5a668de9c3b4787a84f9ed8194d439a", "42ec03202f3c44158a9d8927236c4694"]}, "id": "27b5c601", "outputId": "81549d7c-53cc-4945-ca88-b5a49d818cb6"}, "outputs": [{"output_type": "stream", "name": "stderr", "text": ["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n", "The secret `HF_TOKEN` does not exist in your Colab secrets.\n", "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n", "You will be able to reuse this secret in all of your notebooks.\n", "Please note that authentication is recommended but still optional to access public models or datasets.\n", "  warnings.warn(\n"]}, {"output_type": "display_data", "data": {"text/plain": ["README.md: 0.00B [00:00, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "822b06c6edad46aa97086a8eb24b529f"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["train-00000-of-00001.parquet:   0%|          | 0.00/2.07M [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "e2a73548943447fa8b1c7d6a2cc9f0dd"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["test-00000-of-00001.parquet:   0%|          | 0.00/1.11M [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "05e9876fc12f4c86adaf0b16ea960cf1"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["Generating train split:   0%|          | 0/31962 [00:00<?, ? examples/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "258542da13d74058954ff9113cc68e4b"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["Generating test split:   0%|          | 0/17197 [00:00<?, ? examples/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "84582b5aa04e4b079f829fc475827b97"}}, "metadata": {}}], "source": ["import torch\n", "import datasets\n", "\n", "dataset = datasets.load_dataset('tweets_hate_speech_detection')"]}, {"cell_type": "code", "execution_count": null, "id": "9b66ea76", "metadata": {"ExecuteTime": {"end_time": "2023-08-06T06:29:59.004447Z", "start_time": "2023-08-06T06:29:58.840015Z"}, "id": "9b66ea76", "outputId": "1dafbced-b407-4ea1-f93b-2e4aa5e3230b"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["[nltk_data] Downloading package stopwords to /home/smaug/nltk_data...\n", "[nltk_data]   Package stopwords is already up-to-date!\n"]}], "source": ["# Para simplicidad quitemos characteres pero mantegamos @ y #\n", "import re\n", "import nltk\n", "from nltk.corpus import stopwords\n", "from nltk.stem.snowball import SnowballStemmer\n", "\n", "nltk.download('stopwords')\n", "ss = SnowballStemmer('english')\n", "sw = stopwords.words('english')\n", "\n", "#def split_tokens(row):                             # PASO\n", "#    row['all_tokens'] = [ss.stem(i) for i in       # 5\n", "#                     re.split(r\" +\",               # 3\n", "#                     re.sub(r\"[^a-z@# ]\", \"\",      # 2\n", "#                            row['tweet'].lower())) # 1\n", "#                     if (i not in sw) and len(i)]  # 4\n", "#    return row\n", "\n", "def split_tokens(row):\n", "    # 1- Pasar a minusculas\n", "    tweet_lower = row['tweet'].lower()\n", "\n", "    # 2- Quitar todos los simbolos diferentes de a-z@#\n", "    tweet_cleaned = re.sub(r\"[^a-z@# ]\", \"\", tweet_lower)\n", "\n", "    # 3- Separar en espacios\n", "    tweet_tokens = re.split(r\" +\", tweet_cleaned)\n", "\n", "    # 4- Quitar \"stopword\" y tokens vac\u00edos\n", "    # 5- Aplicar snowball stemmer al resto\n", "    filtered_tokens = [ss.stem(i) for i in tweet_tokens if (i not in sw) and len(i)]\n", "\n", "\n", "    row['all_tokens'] = filtered_tokens\n", "\n", "    return row"]}, {"cell_type": "code", "execution_count": null, "id": "186eecad", "metadata": {"ExecuteTime": {"end_time": "2023-08-06T06:29:59.025478Z", "start_time": "2023-08-06T06:29:59.004447Z"}, "id": "186eecad"}, "outputs": [], "source": ["# Determinamos el vocabulario\n", "dataset = dataset.map(split_tokens)"]}, {"cell_type": "markdown", "id": "686a92c6", "metadata": {"ExecuteTime": {"end_time": "2023-08-06T01:41:00.383114Z", "start_time": "2023-08-06T01:41:00.367813Z"}, "id": "686a92c6"}, "source": ["Ahora podemos crear algunas variables que nos ser\u00e1n \u00fatiles en futuros pasos. Adem\u00e1s, debemos quitar los tokens que ocurren menos de 10 veces para reducir el tama\u00f1o del vocabulario"]}, {"cell_type": "code", "execution_count": null, "id": "39aed35a-414e-4b55-ba66-142714374157", "metadata": {"id": "39aed35a-414e-4b55-ba66-142714374157"}, "outputs": [], "source": ["# print( [k for k,v in counts.items()])"]}, {"cell_type": "code", "execution_count": null, "id": "c0837b22", "metadata": {"ExecuteTime": {"end_time": "2023-08-06T06:29:59.486408Z", "start_time": "2023-08-06T06:29:59.025478Z"}, "deletable": false, "nbgrader": {"cell_type": "code", "checksum": "0d1e8d5c6985acc13aa5f87a951182e5", "grade": false, "grade_id": "cell-0305720eb97e48ae", "locked": false, "schema_version": 3, "solution": true, "task": false}, "id": "c0837b22", "outputId": "74c49a61-211e-4874-f5df-da010be4de86"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Vocab size: 3085\n"]}], "source": ["from collections import Counter\n", "\n", "# Total de palabras\n", "\n", "counts = Counter([i for s in dataset['train']['all_tokens'] for i in s])\n", "counts = {k:v for k, v in counts.items() if v>10} # Filtering\n", "\n", "# Aprox 1 linea para obtener los tokens unicos\n", "vocab = [k for k,v in counts.items()]\n", "# Hint: Use list de python\n", "# Hint2: Use la variable counts\n", "# YOUR CODE HERE\n", "# raise NotImplementedError()\n", "\n", "# Aprox 1 linea para determinar el tama\u00f1o del vocabulario\n", "vocab_size = len(vocab)\n", "n_v = vocab_size\n", "print(\"Vocab size:\", vocab_size)\n", "# YOUR CODE HERE\n", "#raise NotImplementedError()\n", "\n", "# Aprox 2 lineas para definir\n", "#     los diccionarios para ir de un token a un id num\u00e9rico y viceversa\n", "id2tok = {ID:k for ID,k in enumerate(vocab)}\n", "tok2id = {k:ID for ID,k in enumerate(vocab)}\n", "# Hint: Puede que dict y enumerate le sirva para una definici\u00f3n\n", "# YOUR CODE HERE\n", "# raise NotImplementedError()\n", "\n", "# Funcion para quitar tokens \"raros\"\n", "def remove_rare_tokens(row):\n", "    row['tokens'] = [t for t in row['all_tokens'] if t in vocab]\n", "    return row\n", "\n", "dataset = dataset.map(remove_rare_tokens)"]}, {"cell_type": "code", "execution_count": null, "id": "f1d8a6c3", "metadata": {"ExecuteTime": {"end_time": "2023-08-06T06:29:59.501418Z", "start_time": "2023-08-06T06:29:59.487438Z"}, "id": "f1d8a6c3", "outputId": "4915e987-0e5b-43d5-c22d-f37daed57dbc"}, "outputs": [{"data": {"text/plain": ["DatasetDict({\n", "    train: Dataset({\n", "        features: ['label', 'tweet', 'all_tokens', 'tokens'],\n", "        num_rows: 31962\n", "    })\n", "    test: Dataset({\n", "        features: ['label', 'tweet', 'all_tokens', 'tokens'],\n", "        num_rows: 17197\n", "    })\n", "})"]}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}], "source": ["dataset"]}, {"cell_type": "code", "execution_count": null, "id": "d7c79775", "metadata": {"ExecuteTime": {"end_time": "2023-08-06T06:29:59.517184Z", "start_time": "2023-08-06T06:29:59.502434Z"}, "deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "d4cf221bfa47a4f32352470e32c09b04", "grade": true, "grade_id": "cell-801ad99c67585892", "locked": true, "points": 18, "schema_version": 3, "solution": false, "task": false}, "id": "d7c79775", "outputId": "ea78e19c-4982-4a49-8d33-58167931c325"}, "outputs": [{"data": {"text/html": ["\n", "        <div class=\"alert alert-box alert-success\">\n", "        <h1> <!--{id:\"CORRECTMARK\", marks:\"3\"}--> \n", "         \u2713 [3 marks] \n", "         </h1> </div>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"text/html": ["\n", "        <div class=\"alert alert-box alert-success\">\n", "        <h1> <!--{id:\"CORRECTMARK\", marks:\"3\"}--> \n", "         \u2713 [3 marks] \n", "         </h1> </div>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"text/html": ["\n", "        <div class=\"alert alert-box alert-success\">\n", "        <h1> <!--{id:\"CORRECTMARK\", marks:\"3\"}--> \n", "         \u2713 [3 marks] \n", "         </h1> </div>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"text/html": ["\n", "        <div class=\"alert alert-box alert-success\">\n", "        <h1> <!--{id:\"CORRECTMARK\", marks:\"3\"}--> \n", "         \u2713 [3 marks] \n", "         </h1> </div>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"text/html": ["\n", "        <div class=\"alert alert-box alert-success\">\n", "        <h1> <!--{id:\"CORRECTMARK\", marks:\"3\"}--> \n", "         \u2713 [3 marks] \n", "         </h1> </div>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"text/html": ["\n", "        <div class=\"alert alert-box alert-success\">\n", "        <h1> <!--{id:\"CORRECTMARK\", marks:\"3\"}--> \n", "         \u2713 [3 marks] \n", "         </h1> </div>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["with tick.marks(3):\n", "    assert(check_scalar(len(counts), '0xf4f4eb83'))\n", "\n", "with tick.marks(3):\n", "    assert(check_scalar(len(id2tok), '0xf4f4eb83'))\n", "\n", "with tick.marks(3):\n", "    assert(check_scalar(len(vocab), '0xf4f4eb83'))\n", "\n", "with tick.marks(3):\n", "    assert(check_scalar(n_v, '0xf4f4eb83'))\n", "\n", "with tick.marks(3):\n", "    assert(check_scalar(tok2id['father'], '0xb44c37ea'))\n", "\n", "with tick.marks(3):\n", "    assert(check_string(id2tok[1], '0xcf2531b8'))"]}, {"cell_type": "markdown", "id": "67f310c6", "metadata": {"id": "67f310c6"}, "source": ["Ahora, recordemos que Word2Vec ayuda a representar una palabra por su contexto, para ello necesitamos definir una ventana movil (sliding window) que se usa dentro del algoritmo. Esta consiste en tomar cada palabra de una frase, y luego se parea con las N palabras m\u00e1s cercanas (hacia la derecha e izquierda). Por ejemplo, consideremos una frase como \"every good dog does fine\", con una ventana de 2. El resultado ser\u00eda algo como:\n", "\n", "`(every, good)`\n", "`(every, dog)`\n", "`(good, every)`\n", "`(good, dog)`\n", "`(good, does)`\n", "`(dog, every)`\n", "`(dog, good)`\n", "`...`\n", "\n", "Y as\u00ed consecutivamente. La frase u oraci\u00f3n, es convertida en un par `target, context` donde el contex es una lista de tokens dentro de la ventana.\n", "\n", "Luego, definiremos el DataSet usando las clases correspondiente como lo hemos hecho antes."]}, {"cell_type": "code", "execution_count": null, "id": "6dc3dfec", "metadata": {"ExecuteTime": {"end_time": "2023-08-06T06:29:59.580001Z", "start_time": "2023-08-06T06:29:59.520187Z"}, "deletable": false, "nbgrader": {"cell_type": "code", "checksum": "f3be587ebad6002016b9a1210d7dfb2d", "grade": false, "grade_id": "cell-7d04d85a04e5fc7a", "locked": false, "schema_version": 3, "solution": true, "task": false}, "scrolled": true, "id": "6dc3dfec"}, "outputs": [], "source": ["#def windowizer(row, wsize=3):\n", "#    \"\"\"\n", "#    Windowizer function for Word2Vec. Converts sentence to sliding-window\n", "#    pairs.\n", "#    \"\"\"\n", "#    doc = row['tokens']\n", "#    #wsize = 3\n", "#    out = []\n", "#    for i, word in enumerate(doc):\n", "#        target = tok2id[word]\n", "#        window = [i+j for j in\n", "#                  range(-wsize, wsize+1, 1)\n", "#                  if (i+j>=0) &\n", "#                     (i+j<len(doc)) &\n", "#                     (j!=0)]\n", "#\n", "#        out += [(target, tok2id[doc[w]]) for w in window]\n", "#    row['moving_window'] = out\n", "#    return row\n", "\n", "\n", "def windowizer(row, wsize=3):\n", "    \"\"\"\n", "    Windowizer function for Word2Vec. Converts sentence to sliding-window\n", "    pairs.\n", "    \"\"\"\n", "    doc = row['tokens']\n", "    out = []\n", "\n", "    for i, word in enumerate(doc):\n", "        target = tok2id[word]\n", "\n", "        # 1 - Definimos el rango de la ventana movil\n", "        window = [i + j for j in range(-wsize, wsize + 1, 1) if (i + j >= 0) and (i + j < len(doc)) and (j != 0)]\n", "        # print(window)\n", "\n", "        # 2 - Creamos pares de la ventana movil\n", "        # Aprox 1 linea\n", "        window_pairs = [(target, w) for w in window]\n", "        # YOUR CODE HERE\n", "        # raise NotImplementedError()\n", "\n", "        # 3 - Agregamos los pares a la lista de salida\n", "        # Aprox 1 linea\n", "        out += window_pairs\n", "        # YOUR CODE HERE\n", "        # raise NotImplementedError()\n", "\n", "    # 4 - Asingamos el \"movin_window\" a la fila\n", "    row['moving_window'] = out\n", "\n", "    return row\n", "\n", "dataset = dataset.map(windowizer)"]}, {"cell_type": "code", "execution_count": null, "id": "dc357ed4", "metadata": {"ExecuteTime": {"end_time": "2023-08-06T06:30:02.539396Z", "start_time": "2023-08-06T06:30:02.518455Z"}, "deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "726e16c824a2e5b29968e3f1bbab59bf", "grade": true, "grade_id": "cell-66be60a252d35f9d", "locked": true, "points": 5, "schema_version": 3, "solution": false, "task": false}, "id": "dc357ed4", "outputId": "593b43dd-6f75-4467-e9d4-c4d20126b973"}, "outputs": [{"data": {"text/html": ["\n", "        <div class=\"alert alert-box alert-success\">\n", "        <h1> <!--{id:\"CORRECTMARK\", marks:\"5\"}--> \n", "         \u2713 [5 marks] \n", "         </h1> </div>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["with tick.marks(5):\n", "    assert(check_scalar(dataset[\"train\"].num_rows, '0xcd61d16b'))"]}, {"cell_type": "code", "execution_count": null, "id": "de5447bf", "metadata": {"ExecuteTime": {"end_time": "2023-08-06T06:30:02.555506Z", "start_time": "2023-08-06T06:30:02.539396Z"}, "deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "8c728e41718e96a23aa6a0f9ce216263", "grade": false, "grade_id": "cell-1d4097cccc4ceee4", "locked": true, "schema_version": 3, "solution": false, "task": false}, "id": "de5447bf"}, "outputs": [], "source": ["#from torch.utils.data import Dataset, DataLoader\n", "import torch\n", "from torch.utils.data import TensorDataset, Dataset, DataLoader\n", "\n", "\n", "class Word2VecDataset(Dataset):\n", "    def __init__(self, dataset, vocab_size, wsize=3):\n", "        self.dataset = dataset\n", "        self.vocab_size = vocab_size\n", "        self.data = [i for s in dataset['moving_window'] for i in s]\n", "\n", "    def __len__(self):\n", "        return len(self.data)\n", "\n", "    def __getitem__(self, idx):\n", "        return self.data[idx][0], self.data[idx][1]\n"]}, {"cell_type": "markdown", "id": "f6e73cd3", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "9fa751aadaad5be9ec05ab3c4f6e31a4", "grade": false, "grade_id": "cell-249f95b6dcc39bde", "locked": true, "schema_version": 3, "solution": false, "task": false}, "id": "f6e73cd3"}, "source": ["Ahora definiremos dos variables globales, el `BATCH_SIZE` y `N_LOADER_PROCS`.\n", "\n", "`BATCH_SIZE` es el n\u00famero de observaciones devueltas con cada llamada. Gran parte de las aceleraciones del procesamiento de GPU provienen de c\u00e1lculos de matriz por batches masivos. Al elegir el tama\u00f1o del batch, recuerden que generalmente se trata de un trade-off entre el uso de VRAM y la velocidad, excepto cuando el Data Loader en s\u00ed es el cuello de botella. Para acelerar el DataLoader, podemos pasar un argumento a num_workers para habilitar la paralelizaci\u00f3n en la preparaci\u00f3n y carga de datos."]}, {"cell_type": "code", "execution_count": null, "id": "2f74f352", "metadata": {"ExecuteTime": {"end_time": "2023-08-06T06:30:05.406175Z", "start_time": "2023-08-06T06:30:02.556990Z"}, "deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "de6b1867f45bd4673c2fe7c464049ae0", "grade": false, "grade_id": "cell-12cd6ba3b1e9f944", "locked": true, "schema_version": 3, "solution": false, "task": false}, "id": "2f74f352"}, "outputs": [], "source": ["\n", "# Create an instance of the Word2VecDataset\n", "word2vec_dataset_ = Word2VecDataset(dataset['train'], vocab_size=n_v)\n", "\n", "# Convert the Word2VecDataset into a TensorDataset\n", "word2vec_dataset = TensorDataset(torch.tensor(word2vec_dataset_.data, dtype=torch.long))\n", "\n", "BATCH_SIZE = 2**16\n", "N_LOADER_PROCS = 5\n", "\n", "dataloader_train = DataLoader(word2vec_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=N_LOADER_PROCS)"]}, {"cell_type": "markdown", "id": "c53220df", "metadata": {"id": "c53220df"}, "source": ["### Paso 2 - Construyendo la Red\n", "La arquitectura que usaremos para esta ocasi\u00f3n ser\u00e1 la dada por una versi\u00f3n de Word2Vec, esta consiste en:\n", "* Tres capas: Input, hidden y output\n", "* Tanto el tama\u00f1o de la input como la output son del tama\u00f1o del vocabulario. Pero la hidden es un poco m\u00e1s peque\u00f1a\n", "* Todas son Fully Connected con Funciones de Activaci\u00f3n Lineales\n", "\n", "Como mencionamos en clase hay dos variantes\n", "* CBOW (Continuous Bag of Words): El enfoque est\u00e1 dado en las palabras de contexto para dar \u00e9nfasis a la palabra central. O en otras palabras, las palabras de contexto son el input y la palabra central son el output (Espero que esto haga m\u00e1s sentido de la explicaci\u00f3n en clase)\n", "* Skip-gram: La palabra central es el input, y las de contexto son la salida.\n", "\n", "Definamos CBOW para este laboratorio...\n", "\n", "Pero antes, debemos encodear nuestras palabras (otra vez como lo hicimos en el laboratorio pasado), esta implementaci\u00f3n es similar a la que hicimos anteriormente, pero observen el uso de tensores."]}, {"cell_type": "code", "execution_count": null, "id": "b7927273", "metadata": {"ExecuteTime": {"end_time": "2023-08-06T06:30:05.419716Z", "start_time": "2023-08-06T06:30:05.412780Z"}, "deletable": false, "nbgrader": {"cell_type": "code", "checksum": "79bcbc0e794ad778277522e74da6198f", "grade": false, "grade_id": "cell-719345b22d8a6412", "locked": false, "schema_version": 3, "solution": true, "task": false}, "id": "b7927273"}, "outputs": [], "source": ["from torch import nn\n", "\n", "size = 20\n", "input_ = 7\n", "\n", "def one_hot_encode(input_, size):\n", "    vec = torch.zeros(size).float()\n", "    # Aprox 1 linea para\n", "    vec[input_] = 1\n", "    # YOUR CODE HERE\n", "    # raise NotImplementedError()\n", "    return vec\n", "\n", "ohe = one_hot_encode(input_, size)\n", "linear_layer = nn.Linear(size, 1, bias=False)"]}, {"cell_type": "code", "execution_count": null, "id": "afda896e", "metadata": {"ExecuteTime": {"end_time": "2023-08-06T06:30:05.426787Z", "start_time": "2023-08-06T06:30:05.419716Z"}, "deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "b9076aa87b82f56230dfbf2cec1760ad", "grade": true, "grade_id": "cell-87b60412b0ba69d1", "locked": true, "points": 3, "schema_version": 3, "solution": false, "task": false}, "id": "afda896e", "outputId": "51d9fc2f-a3af-458c-ddeb-437ec9a23859"}, "outputs": [{"data": {"text/html": ["\n", "        <div class=\"alert alert-box alert-success\">\n", "        <h1> <!--{id:\"CORRECTMARK\", marks:\"3\"}--> \n", "         \u2713 [3 marks] \n", "         </h1> </div>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["with tick.marks(3):\n", "    assert int(ohe[7])  == 1"]}, {"cell_type": "markdown", "id": "245325be", "metadata": {"id": "245325be"}, "source": ["Ahora, sobreescribamos el comportamiento natural de la inicializacion de pesos, para que estos en lugar de iniciar aleatoriamente, sean valores de 0 - size. Esto lo hacemos dentro `torch.no_grad()` para quitar el tracking de la gradiente (recuerden que cuando usamos los tensores de PyTorch la gradiente se le hace tracking, es decir que se almacenan para hacer la diferenciar la p\u00e9rdida con respecto de cada parametro en el modelo. Debido a que en esta ocasion lo estamos seteando manualmente no queremos que se almacene y sea considerado en futuras backpropagations.\n", "\n", "Observen como al pasar el vector encodeado a la capa nos devuelve efectivamente el n\u00famero que corresponde en `linear_layer(ohe)`"]}, {"cell_type": "code", "execution_count": null, "id": "8fbbf1cb", "metadata": {"ExecuteTime": {"end_time": "2023-08-06T06:30:05.440557Z", "start_time": "2023-08-06T06:30:05.426787Z"}, "deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "7d26f4b5566a553c62c142900b507a20", "grade": false, "grade_id": "cell-4c85a0ffbffa8e55", "locked": true, "schema_version": 3, "solution": false, "task": false}, "id": "8fbbf1cb", "outputId": "20e723b5-b73d-4776-ef25-a48a620f6f5b"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Parameter containing:\n", "tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n", "         14., 15., 16., 17., 18., 19.]], requires_grad=True)\n", "tensor([7.], grad_fn=<SqueezeBackward4>)\n"]}], "source": ["with torch.no_grad():\n", "    linear_layer.weight = nn.Parameter(\n", "        torch.arange(size, dtype=torch.float).reshape(linear_layer.weight.shape))\n", "\n", "print(linear_layer.weight)\n", "print(linear_layer(ohe))"]}, {"cell_type": "markdown", "id": "56a7ee35", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "d9a663115c0df77e2c807969aac5ba8e", "grade": false, "grade_id": "cell-71d644e296aee563", "locked": true, "schema_version": 3, "solution": false, "task": false}, "id": "56a7ee35"}, "source": ["Ya que tenemos un mejor entendimiento de este tipo de layers en Word2Vec, debemos saber que PyTorch tiene una implementaci\u00f3n m\u00e1s eficiente usando `nn.Embedding`, el cual toma los \u00edndices de input y regresa el peso del borde correspondiente a ese \u00edndice.\n", "\n", "Un equivalente a lo que hemos hecho anteriormente ser\u00eda lo que se presenta en la siguiente celda.\n", "\n", "Noten como volvemos a obtener un tensor similar al que obtuvimos antes."]}, {"cell_type": "code", "execution_count": null, "id": "c53403aa", "metadata": {"ExecuteTime": {"end_time": "2023-08-06T06:30:05.463215Z", "start_time": "2023-08-06T06:30:05.440557Z"}, "deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "bc320f7bb25be19e4e0033321279d77c", "grade": false, "grade_id": "cell-bf93477666a5691e", "locked": true, "schema_version": 3, "solution": false, "task": false}, "id": "c53403aa", "outputId": "1b5987b8-5bac-4a38-d87a-2072b1827842"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Parameter containing:\n", "tensor([[ 0.],\n", "        [ 1.],\n", "        [ 2.],\n", "        [ 3.],\n", "        [ 4.],\n", "        [ 5.],\n", "        [ 6.],\n", "        [ 7.],\n", "        [ 8.],\n", "        [ 9.],\n", "        [10.],\n", "        [11.],\n", "        [12.],\n", "        [13.],\n", "        [14.],\n", "        [15.],\n", "        [16.],\n", "        [17.],\n", "        [18.],\n", "        [19.]], requires_grad=True)\n", "tensor([7.], grad_fn=<EmbeddingBackward0>)\n"]}], "source": ["embedding_layer = nn.Embedding(size, 1)\n", "\n", "with torch.no_grad():\n", "    embedding_layer.weight = nn.Parameter(\n", "        torch.arange(size, dtype=torch.float\n", "        ).reshape(embedding_layer.weight.shape))\n", "\n", "print(embedding_layer.weight)\n", "print(embedding_layer(torch.tensor(input_)))"]}, {"cell_type": "markdown", "id": "7610c2f8", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "3bcc21a605eb207c161b41a6ded571ad", "grade": false, "grade_id": "cell-508a31edfe7f0cfc", "locked": true, "schema_version": 3, "solution": false, "task": false}, "id": "7610c2f8"}, "source": ["Con esto en consideraci\u00f3n, es momento de implementar nuestro modelo Word2Vec.\n", "\n", "Noten el embedding_size, este corresponde a la cantidad de representaciones de cada palabra, como dijimos en clase, esto ser\u00eda la cantidad de funciones de activaciones con las que trabajaremos.\n", "\n", "Adem\u00e1s, consideren las siguientes explicaciones\n", "\n", "`self.embed`: Es una capa de embedding para convertir la entrada (el \u00edndice del token de centro/contexto) en la codificaci\u00f3n one-hot, y luego recuperar los pesos correspondientes a estos \u00edndices en la capa hidden de menor dimensi\u00f3n.\n", "\n", "`self.expand`: Es una capa lineal para predecir la probabilidad de una palabra de centro/contexto dada la hidden layer. Deshabilitamos el bias (la intercepci\u00f3n) porque cambiamos la escala de nuestras predicciones de todos modos.\n", "\n", "`logits`: Este vuelve a expandir la capa hidden para hacer predicciones. Estas predicciones sin procesar deben volver a escalarse con softmax, pero omitimos este paso aqu\u00ed, ya que PyTorch implementa los pasos relevantes en la Cross Entropy loss."]}, {"cell_type": "code", "execution_count": null, "id": "159c2e13", "metadata": {"ExecuteTime": {"end_time": "2023-08-06T06:30:05.479149Z", "start_time": "2023-08-06T06:30:05.465210Z"}, "id": "159c2e13"}, "outputs": [], "source": ["class Word2Vec(nn.Module):\n", "    def __init__(self, vocab_size, embedding_size):\n", "        super().__init__()\n", "        self.embed = nn.Embedding(vocab_size, embedding_size)\n", "        self.expand = nn.Linear(embedding_size, vocab_size, bias=False)\n", "\n", "    def forward(self, input_):\n", "        # Pasamos el input a una representaci\u00f3n m\u00e1s peque\u00f1a\n", "        hidden = self.embed(input_)\n", "        # Expandemos hacia las predicciones\n", "        logits = self.expand(hidden)\n", "        return logits"]}, {"cell_type": "markdown", "id": "9b549e68", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "d2424728d269e4533722e107620b3f37", "grade": false, "grade_id": "cell-821b7f6886ebdd0d", "locked": true, "schema_version": 3, "solution": false, "task": false}, "id": "9b549e68"}, "source": ["### Paso 3 - Entrenamiento (Training)\n", "\n", "\n", "El entrenamiento en el contexto de las redes neuronales significa hacer predicciones repetidamente utilizando las observaciones en el conjunto de datos y luego ajustar los par\u00e1metros para corregir el error en las predicciones.\n", "\n", "Debido a que no queremos que la red aprenda perfectamente la predicci\u00f3n m\u00e1s reciente mientras olvida todas las dem\u00e1s predicciones, generalmente le damos un \"learning rate\", que es una penalizaci\u00f3n en el ajuste de p\u00e9rdida para evitar que se ajuste solo a la observaci\u00f3n m\u00e1s reciente. (Recuerden como funciona backpropgation)\n", "\n", "Cuanto m\u00e1s tiempo entrenemos la red, con mayor perfecci\u00f3n aprender\u00e1 los datos de entrenamiento, pero a menudo esto conlleva el riesgo de overfitting y no poder generalizar a datos no vistos. Sin embargo, dado que con Word2Vec nuestro objetivo no es inferir datos no vistos, sino describir datos \"vistos\", \u00bfcu\u00e1l creen que es la implicaci\u00f3n del overfitting en este tipo de modelos? (M\u00e1s adelante se deja nuevamente la pregunta para que sea respondida)"]}, {"cell_type": "code", "execution_count": null, "id": "0b5522dc", "metadata": {"ExecuteTime": {"end_time": "2023-08-06T22:14:29.330767Z", "start_time": "2023-08-06T22:14:29.296636Z"}, "deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "1a82d8c2a18f8548fa5a4b5765fb2fc1", "grade": false, "grade_id": "cell-588eec0490d68d93", "locked": true, "schema_version": 3, "solution": false, "task": false}, "id": "0b5522dc", "outputId": "97c03e07-3886-4691-d5df-d1ab2ab65fbc"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Using: cuda\n"]}], "source": ["# Algunos hyper parametros\n", "\n", "# Demasiado peque\u00f1o pero es solo para fines de aprendizaje\n", "EMBED_SIZE = 50\n", "model = Word2Vec(n_v, EMBED_SIZE)\n", "\n", "# Traten de usar ya el CUDA si pueden por favor\n", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n", "print(\"Using:\",device)\n", "model.to(device)\n", "\n", "# Otros parametros para el training\n", "LR = 3e-4\n", "EPOCHS = 3\n", "loss_fn = nn.CrossEntropyLoss()\n", "# Noten el tipo de optimizador que estamos usando :)\n", "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)"]}, {"cell_type": "code", "execution_count": null, "id": "784bfe0c", "metadata": {"ExecuteTime": {"end_time": "2023-08-06T22:56:25.851356Z", "start_time": "2023-08-06T22:14:39.282185Z"}, "deletable": false, "nbgrader": {"cell_type": "code", "checksum": "469a6891ae3c2d3ef10bfe706471264f", "grade": false, "grade_id": "cell-8a89fd3a288c223a", "locked": false, "schema_version": 3, "solution": true, "task": false}, "scrolled": true, "id": "784bfe0c", "outputId": "9965d010-fafe-4d93-838b-1b2b5c97cec1"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Working with batch 0\n", "Done working with element 0\n", "Done working with element 6500\n", "Done working with element 13000\n", "Done working with element 19500\n", "Done working with element 26000\n", "Done working with element 32500\n", "Done working with element 39000\n", "Done working with element 45500\n", "Done working with element 52000\n", "Done working with element 58500\n", "Done working with element 65000\n", "Working with batch 1\n", "Done working with element 0\n", "Done working with element 6500\n", "Done working with element 13000\n", "Done working with element 19500\n", "Done working with element 26000\n", "Done working with element 32500\n", "Done working with element 39000\n", "Done working with element 45500\n", "Done working with element 52000\n", "Done working with element 58500\n", "Done working with element 65000\n", "Working with batch 2\n", "Done working with element 0\n", "Done working with element 6500\n", "Done working with element 13000\n", "Done working with element 19500\n", "Done working with element 26000\n", "Done working with element 32500\n", "Done working with element 39000\n", "Done working with element 45500\n", "Done working with element 52000\n", "Done working with element 58500\n", "Done working with element 65000\n", "Working with batch 3\n", "Done working with element 0\n", "Done working with element 6500\n", "Done working with element 13000\n", "Done working with element 19500\n", "Done working with element 26000\n", "Done working with element 32500\n", "Done working with element 39000\n", "Done working with element 45500\n", "Done working with element 52000\n", "Done working with element 58500\n", "Done working with element 65000\n", "Working with batch 4\n", "Done working with element 0\n", "Done working with element 6500\n", "Done working with element 13000\n", "Done working with element 19500\n", "Done working with element 26000\n", "Done working with element 32500\n", "Done working with element 39000\n", "Done working with element 45500\n", "Done working with element 52000\n", "Done working with element 58500\n", "Done working with element 65000\n", "Working with batch 5\n", "Done working with element 0\n", "Done working with element 6500\n", "Done working with element 13000\n", "Done working with element 19500\n", "Done working with element 26000\n", "Done working with element 32500\n", "Done working with element 39000\n", "Done working with element 45500\n", "Done working with element 52000\n", "Done working with element 58500\n", "Done working with element 65000\n", "Working with batch 6\n", "Done working with element 0\n", "Done working with element 6500\n", "Done working with element 13000\n", "Done working with element 19500\n", "Done working with element 26000\n", "Done working with element 32500\n", "Done working with element 39000\n", "Done working with element 45500\n", "Done working with element 52000\n", "Done working with element 58500\n", "Done working with element 65000\n", "Working with batch 7\n", "Done working with element 0\n", "Done working with element 6500\n", "Done working with element 13000\n", "Done working with element 19500\n", "Done working with element 26000\n", "Done working with element 32500\n", "Done working with element 39000\n", "Done working with element 45500\n", "Done working with element 52000\n", "Done working with element 58500\n", "Done working with element 65000\n", "Working with batch 8\n", "Done working with element 0\n", "Done working with element 6500\n", "Done working with element 13000\n", "Done working with element 19500\n", "Done working with element 26000\n", "Done working with element 32500\n", "Done working with element 39000\n", "Done working with element 45500\n", "Done working with element 52000\n", "Done working with element 58500\n", "Done working with element 65000\n", "Working with batch 9\n", "Done working with element 0\n", "Done working with element 6500\n", "Done working with element 13000\n", "Done working with element 19500\n", "Done working with element 26000\n", "Done working with element 32500\n", "Done working with element 39000\n", "Done working with element 45500\n", "Done working with element 52000\n", "Done working with element 58500\n", "Done working with element 65000\n", "Working with batch 10\n", "Done working with element 0\n", "Done working with element 6500\n", "Done working with element 13000\n", "Done working with element 19500\n", "Done working with element 26000\n", "Done working with element 32500\n", "Done working with element 39000\n", "Done working with element 45500\n", "Done working with element 52000\n", "Done working with element 58500\n", "Done working with element 65000\n", "Working with batch 11\n", "Done working with element 0\n", "Done working with element 6500\n", "Done working with element 13000\n", "Done working with element 19500\n", "Done working with element 26000\n", "Done working with element 32500\n", "Done working with element 39000\n", "Done working with element 45500\n", "Done working with element 52000\n", "Done working with element 58500\n", "Done working with element 65000\n", "Working with batch 12\n", "Done working with element 0\n", "Done working with element 6500\n", "Done working with element 13000\n", "Done working with element 19500\n", "Done working with element 26000\n", "Done working with element 32500\n", "Done working with element 39000\n", "Done working with element 45500\n", "Done working with element 52000\n", "Done working with element 58500\n", "Done working with element 65000\n", "Working with batch 13\n", "Done working with element 0\n", "Epoca 0, loss: 7.038302417290818\n", "Working with batch 0\n", "Done working with element 0\n", "Done working with element 6500\n", "Done working with element 13000\n", "Done working with element 19500\n", "Done working with element 26000\n", "Done working with element 32500\n", "Done working with element 39000\n", "Done working with element 45500\n", "Done working with element 52000\n", "Done working with element 58500\n", "Done working with element 65000\n", "Working with batch 1\n", "Done working with element 0\n", "Done working with element 6500\n", "Done working with element 13000\n", "Done working with element 19500\n", "Done working with element 26000\n", "Done working with element 32500\n", "Done working with element 39000\n", "Done working with element 45500\n", "Done working with element 52000\n", "Done working with element 58500\n", "Done working with element 65000\n", "Working with batch 2\n", "Done working with element 0\n", "Done working with element 6500\n", "Done working with element 13000\n", "Done working with element 19500\n", "Done working with element 26000\n", "Done working with element 32500\n", "Done working with element 39000\n", "Done working with element 45500\n", "Done working with element 52000\n", "Done working with element 58500\n", "Done working with element 65000\n", "Working with batch 3\n", "Done working with element 0\n", "Done working with element 6500\n", "Done working with element 13000\n", "Done working with element 19500\n", "Done working with element 26000\n", "Done working with element 32500\n", "Done working with element 39000\n", "Done working with element 45500\n", "Done working with element 52000\n", "Done working with element 58500\n", "Done working with element 65000\n", "Working with batch 4\n", "Done working with element 0\n", "Done working with element 6500\n", "Done working with element 13000\n", "Done working with element 19500\n", "Done working with element 26000\n", "Done working with element 32500\n", "Done working with element 39000\n", "Done working with element 45500\n", "Done working with element 52000\n", "Done working with element 58500\n", "Done working with element 65000\n", "Working with batch 5\n", "Done working with element 0\n", "Done working with element 6500\n", "Done working with element 13000\n", "Done working with element 19500\n", "Done working with element 26000\n", "Done working with element 32500\n", "Done working with element 39000\n", "Done working with element 45500\n", "Done working with element 52000\n", "Done working with element 58500\n", "Done working with element 65000\n", "Working with batch 6\n", "Done working with element 0\n", "Done working with element 6500\n", "Done working with element 13000\n", "Done working with element 19500\n", "Done working with element 26000\n", "Done working with element 32500\n", "Done working with element 39000\n", "Done working with element 45500\n", "Done working with element 52000\n", "Done working with element 58500\n", "Done working with element 65000\n", "Working with batch 7\n", "Done working with element 0\n", "Done working with element 6500\n", "Done working with element 13000\n", "Done working with element 19500\n", "Done working with element 26000\n", "Done working with element 32500\n", "Done working with element 39000\n", "Done working with element 45500\n", "Done working with element 52000\n", "Done working with element 58500\n", "Done working with element 65000\n", "Working with batch 8\n", "Done working with element 0\n", "Done working with element 6500\n", "Done working with element 13000\n", "Done working with element 19500\n", "Done working with element 26000\n", "Done working with element 32500\n", "Done working with element 39000\n", "Done working with element 45500\n", "Done working with element 52000\n", "Done working with element 58500\n", "Done working with element 65000\n", "Working with batch 9\n", "Done working with element 0\n", "Done working with element 6500\n", "Done working with element 13000\n", "Done working with element 19500\n", "Done working with element 26000\n", "Done working with element 32500\n", "Done working with element 39000\n", "Done working with element 45500\n", "Done working with element 52000\n", "Done working with element 58500\n", "Done working with element 65000\n", "Working with batch 10\n", "Done working with element 0\n", "Done working with element 6500\n", "Done working with element 13000\n", "Done working with element 19500\n", "Done working with element 26000\n", "Done working with element 32500\n", "Done working with element 39000\n", "Done working with element 45500\n", "Done working with element 52000\n", "Done working with element 58500\n", "Done working with element 65000\n", "Working with batch 11\n", "Done working with element 0\n", "Done working with element 6500\n", "Done working with element 13000\n", "Done working with element 19500\n", "Done working with element 26000\n", "Done working with element 32500\n", "Done working with element 39000\n", "Done working with element 45500\n", "Done working with element 52000\n", "Done working with element 58500\n", "Done working with element 65000\n", "Working with batch 12\n", "Done working with element 0\n", "Done working with element 6500\n", "Done working with element 13000\n", "Done working with element 19500\n", "Done working with element 26000\n", "Done working with element 32500\n", "Done working with element 39000\n", "Done working with element 45500\n", "Done working with element 52000\n", "Done working with element 58500\n", "Done working with element 65000\n", "Working with batch 13\n", "Done working with element 0\n", "Epoca 1, loss: 7.016921347231394\n", "Working with batch 0\n", "Done working with element 0\n", "Done working with element 6500\n", "Done working with element 13000\n", "Done working with element 19500\n", "Done working with element 26000\n", "Done working with element 32500\n", "Done working with element 39000\n", "Done working with element 45500\n", "Done working with element 52000\n", "Done working with element 58500\n", "Done working with element 65000\n", "Working with batch 1\n", "Done working with element 0\n", "Done working with element 6500\n", "Done working with element 13000\n", "Done working with element 19500\n", "Done working with element 26000\n", "Done working with element 32500\n", "Done working with element 39000\n", "Done working with element 45500\n", "Done working with element 52000\n", "Done working with element 58500\n", "Done working with element 65000\n", "Working with batch 2\n", "Done working with element 0\n", "Done working with element 6500\n", "Done working with element 13000\n", "Done working with element 19500\n", "Done working with element 26000\n", "Done working with element 32500\n", "Done working with element 39000\n", "Done working with element 45500\n", "Done working with element 52000\n", "Done working with element 58500\n", "Done working with element 65000\n", "Working with batch 3\n", "Done working with element 0\n", "Done working with element 6500\n", "Done working with element 13000\n", "Done working with element 19500\n", "Done working with element 26000\n", "Done working with element 32500\n", "Done working with element 39000\n", "Done working with element 45500\n", "Done working with element 52000\n", "Done working with element 58500\n", "Done working with element 65000\n", "Working with batch 4\n", "Done working with element 0\n", "Done working with element 6500\n", "Done working with element 13000\n", "Done working with element 19500\n", "Done working with element 26000\n", "Done working with element 32500\n", "Done working with element 39000\n", "Done working with element 45500\n", "Done working with element 52000\n", "Done working with element 58500\n", "Done working with element 65000\n", "Working with batch 5\n", "Done working with element 0\n", "Done working with element 6500\n", "Done working with element 13000\n", "Done working with element 19500\n", "Done working with element 26000\n", "Done working with element 32500\n", "Done working with element 39000\n", "Done working with element 45500\n", "Done working with element 52000\n", "Done working with element 58500\n", "Done working with element 65000\n", "Working with batch 6\n", "Done working with element 0\n", "Done working with element 6500\n", "Done working with element 13000\n", "Done working with element 19500\n", "Done working with element 26000\n", "Done working with element 32500\n", "Done working with element 39000\n", "Done working with element 45500\n", "Done working with element 52000\n", "Done working with element 58500\n", "Done working with element 65000\n", "Working with batch 7\n", "Done working with element 0\n", "Done working with element 6500\n", "Done working with element 13000\n", "Done working with element 19500\n", "Done working with element 26000\n", "Done working with element 32500\n", "Done working with element 39000\n", "Done working with element 45500\n", "Done working with element 52000\n", "Done working with element 58500\n", "Done working with element 65000\n", "Working with batch 8\n", "Done working with element 0\n", "Done working with element 6500\n", "Done working with element 13000\n", "Done working with element 19500\n", "Done working with element 26000\n", "Done working with element 32500\n", "Done working with element 39000\n", "Done working with element 45500\n", "Done working with element 52000\n", "Done working with element 58500\n", "Done working with element 65000\n", "Working with batch 9\n", "Done working with element 0\n", "Done working with element 6500\n", "Done working with element 13000\n", "Done working with element 19500\n", "Done working with element 26000\n", "Done working with element 32500\n", "Done working with element 39000\n", "Done working with element 45500\n", "Done working with element 52000\n", "Done working with element 58500\n", "Done working with element 65000\n", "Working with batch 10\n", "Done working with element 0\n", "Done working with element 6500\n", "Done working with element 13000\n", "Done working with element 19500\n", "Done working with element 26000\n", "Done working with element 32500\n", "Done working with element 39000\n", "Done working with element 45500\n", "Done working with element 52000\n", "Done working with element 58500\n", "Done working with element 65000\n", "Working with batch 11\n", "Done working with element 0\n", "Done working with element 6500\n", "Done working with element 13000\n", "Done working with element 19500\n", "Done working with element 26000\n", "Done working with element 32500\n", "Done working with element 39000\n", "Done working with element 45500\n", "Done working with element 52000\n", "Done working with element 58500\n", "Done working with element 65000\n", "Working with batch 12\n", "Done working with element 0\n", "Done working with element 6500\n", "Done working with element 13000\n", "Done working with element 19500\n", "Done working with element 26000\n", "Done working with element 32500\n", "Done working with element 39000\n", "Done working with element 45500\n", "Done working with element 52000\n", "Done working with element 58500\n", "Done working with element 65000\n", "Working with batch 13\n", "Done working with element 0\n", "Epoca 2, loss: 7.015815804352222\n"]}], "source": ["import pickle\n", "\n", "SAVE_MODEL = True\n", "LOAD_MODEL = False\n", "MODEL_PATH = \"models/w2v.pth\"\n", "LOSS_PATH = \"models/losses.pkl\"\n", "\n", "running_loss = []\n", "\n", "if LOAD_MODEL:\n", "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n", "    # model = pickle.load(MODEL_PATH)\n", "    model = Word2Vec(vocab_size, EMBED_SIZE)\n", "    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n", "    model.to(device)\n", "    with open(LOSS_PATH, \"rb\") as f:\n", "        running_loss = pickle.load(f)\n", "else:\n", "    running_loss = []\n", "    for epoch in range(EPOCHS):\n", "        epoch_loss = 0\n", "        losses = []\n", "        for ix, batch in enumerate(dataloader_train):\n", "            print(f\"Working with batch {ix}\")\n", "            for i in range(len(batch[0])):\n", "                center = batch[0][i][0]\n", "                context = batch[0][i][1]\n", "                center, context = center.to(device), context.to(device)\n", "                # print(center)\n", "                # Aprox 1 linea para\n", "                # optimizer.zero...\n", "                optimizer.zero_grad()\n", "                # YOUR CODE HERE\n", "                # raise NotImplementedError()\n", "                logits = model(input_=context)\n", "                # print(logits)\n", "                # Aprox 1 linea para\n", "                loss = loss_fn(logits, center)\n", "                # YOUR CODE HERE\n", "                # raise NotImplementedError()\n", "                losses.append(loss.item())\n", "                loss.backward()\n", "                optimizer.step()\n", "\n", "                if i% 6500 == 0:\n", "                    print(f\"Done working with element {i}\")\n", "\n", "        epoch_loss = np.mean(losses)\n", "        running_loss.append(epoch_loss)\n", "\n", "        # Mostrar la perdida cada N epocas\n", "        if epoch % 1 == 0:\n", "            print(f'Epoca {epoch}, loss: {epoch_loss}')\n", "\n", "    if SAVE_MODEL:\n", "        torch.save(model.state_dict(), MODEL_PATH)\n", "        with open(LOSS_PATH, \"wb\") as f:\n", "            pickle.dump(running_loss, f)"]}, {"cell_type": "code", "execution_count": null, "id": "c00fbfdb-4a55-4507-9609-8aa3a8e295b0", "metadata": {"id": "c00fbfdb-4a55-4507-9609-8aa3a8e295b0", "outputId": "29b41da5-30eb-45fa-8cab-952b106e00f3"}, "outputs": [{"data": {"text/plain": ["False"]}, "execution_count": 25, "metadata": {}, "output_type": "execute_result"}], "source": ["compare_numbers(new_representation(running_loss[len(running_loss)-1]), \"3c3d\", '0x1.b000000000000p+2')"]}, {"cell_type": "code", "execution_count": null, "id": "adaeaf72-3db7-4fa2-adf0-5ad37b3367b3", "metadata": {"id": "adaeaf72-3db7-4fa2-adf0-5ad37b3367b3", "outputId": "50930030-8c55-4aec-d7bf-10a43e56d7aa"}, "outputs": [{"data": {"text/plain": ["'0x1.c103204a9cf98p+2'"]}, "execution_count": 26, "metadata": {}, "output_type": "execute_result"}], "source": ["new_representation(running_loss[len(running_loss)-1])\n", "# Error similar al anio pasado, contar las marcas por favor"]}, {"cell_type": "code", "execution_count": null, "id": "fe24f3e7", "metadata": {"ExecuteTime": {"end_time": "2023-08-07T00:08:24.732102Z", "start_time": "2023-08-07T00:08:24.725620Z"}, "deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "32ff5c07dd800c1cc3e4b2446f56cded", "grade": true, "grade_id": "cell-5fe835d2e4773764", "locked": true, "points": 0, "schema_version": 3, "solution": false, "task": false}, "scrolled": true, "id": "fe24f3e7", "outputId": "daee1fdb-32b5-4241-fd0c-fd9574655d90"}, "outputs": [{"data": {"text/html": ["<hr style=\"height:10px;border:none;color:#f00;background-color:#f00;\" />\n", "        <div class=\"alert alert-box alert-danger\">\n", "        <h1> <!--{id:\"WRONGMARK\", marks:\"5\"}--> Test failed \u2718 [0/5] marks  </h1> </div>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}, {"ename": "AssertionError", "evalue": "", "output_type": "error", "traceback": ["\u001b[31m---------------------------------------------------------------------------\u001b[39m", "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)", "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tick.marks(\u001b[32m5\u001b[39m):        \n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m compare_numbers(new_representation(running_loss[\u001b[38;5;28mlen\u001b[39m(running_loss)-\u001b[32m1\u001b[39m]), \u001b[33m\"\u001b[39m\u001b[33m3c3d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m0x1.b000000000000p+2\u001b[39m\u001b[33m'\u001b[39m)\n", "\u001b[31mAssertionError\u001b[39m: "]}], "source": ["with tick.marks(5):\n", "    assert compare_numbers(new_representation(running_loss[len(running_loss)-1]), \"3c3d\", '0x1.b000000000000p+2')"]}, {"cell_type": "code", "execution_count": null, "id": "55ce97f6", "metadata": {"ExecuteTime": {"end_time": "2023-08-06T23:47:02.588400Z", "start_time": "2023-08-06T23:47:02.385345Z"}, "id": "55ce97f6", "outputId": "de1b2b41-213b-4067-83f0-a7aa17251eb2"}, "outputs": [{"data": {"image/png": "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", "text/plain": ["<Figure size 640x480 with 1 Axes>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["# Graficamos la perdida\n", "epoch_ = np.arange(len(running_loss))\n", "plt.figure()\n", "plt.plot(epoch_, running_loss, 'r', label='Loss',)\n", "plt.legend()\n", "plt.xlabel('Epoch'), plt.ylabel('NLL')\n", "plt.show()"]}, {"cell_type": "markdown", "id": "7f0a4424", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "bead7c6fdad67968d9b943584a416c84", "grade": false, "grade_id": "cell-6ba5a104f61555b3", "locked": true, "schema_version": 3, "solution": false, "task": false}, "id": "7f0a4424"}, "source": ["Bueno, hemos visto la cantidad de tiemp que hay que invertirle para entrenar una red tan sencilla como la que se usa en Wor2Vec. En mi caso, usando CUDA le tom\u00f3 alrededor de **42 minutos**. Ahora consideren aquel modelo donde no solo se sacan 50 representaciones de cada palabra sino miles, adem\u00e1s que se entrenan por m\u00e1s epocas, no solo 3.\n", "\n", "Ahora veamos que tipo de palabras son las m\u00e1s cercanas a una peque\u00f1a muestra de 4 palabras. Para esto primero necesitamos sacar los pesos del modelo y pasarlos al cpu para trabajarlos como NumPy Arrays. Luego aplicaremos una funci\u00f3n para encontrar la distancia dada una m\u00e9trica (en este caso la distancia del coseno)."]}, {"cell_type": "code", "execution_count": null, "id": "3acd42a5", "metadata": {"ExecuteTime": {"end_time": "2023-08-07T00:38:03.418312Z", "start_time": "2023-08-07T00:38:03.384629Z"}, "deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "adf71f64fc49e300a12791c23246a50b", "grade": false, "grade_id": "cell-79c39d4b13d3ed81", "locked": true, "schema_version": 3, "solution": false, "task": false}, "id": "3acd42a5"}, "outputs": [], "source": ["wordvecs = model.expand.weight.cpu().detach().numpy()\n", "tokens = ['good', 'bad', 'school', 'day']"]}, {"cell_type": "code", "execution_count": null, "id": "bae64a29", "metadata": {"ExecuteTime": {"end_time": "2023-08-07T00:38:23.499862Z", "start_time": "2023-08-07T00:38:23.360937Z"}, "deletable": false, "nbgrader": {"cell_type": "code", "checksum": "6e097e2559d99154cefeabf845d4aa0d", "grade": false, "grade_id": "cell-1d82d3393549def7", "locked": false, "schema_version": 3, "solution": true, "task": false}, "id": "bae64a29", "outputId": "76ce4a79-607a-45fb-e28a-4327e22c5968"}, "outputs": [{"ename": "NotImplementedError", "evalue": "", "output_type": "error", "traceback": ["\u001b[31m---------------------------------------------------------------------------\u001b[39m", "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)", "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m dmat = get_distance_matrix(wordvecs, \u001b[33m'\u001b[39m\u001b[33mcosine\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28mprint\u001b[39m(word, [t[\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[43mget_k_similar_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdmat\u001b[49m\u001b[43m)\u001b[49m], \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n", "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mget_k_similar_words\u001b[39m\u001b[34m(word, dist_matrix, k)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_k_similar_words\u001b[39m(word, dist_matrix, k=\u001b[32m10\u001b[39m):\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# Aprox 2 lineas para\u001b[39;00m\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# idx = ...\u001b[39;00m\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# dists = ... # Use la funcion dada arriba\u001b[39;00m\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# Hint: tok2id\u001b[39;00m\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# YOUR CODE HERE\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n\u001b[32m     15\u001b[39m     ind = np.argpartition(dists, k)[:k+\u001b[32m1\u001b[39m]\n\u001b[32m     16\u001b[39m     ind = ind[np.argsort(dists[ind])][\u001b[32m1\u001b[39m:]\n", "\u001b[31mNotImplementedError\u001b[39m: "]}], "source": ["from scipy.spatial import distance\n", "import numpy as np\n", "\n", "def get_distance_matrix(wordvecs, metric):\n", "    dist_matrix = distance.squareform(distance.pdist(wordvecs, metric))\n", "    return dist_matrix\n", "\n", "def get_k_similar_words(word, dist_matrix, k=10):\n", "    # Aprox 2 lineas para\n", "    # idx = ...\n", "    # dists = ... # Use la funcion dada arriba\n", "    # Hint: tok2id\n", "    # YOUR CODE HERE\n", "    raise NotImplementedError()\n", "    ind = np.argpartition(dists, k)[:k+1]\n", "    ind = ind[np.argsort(dists[ind])][1:]\n", "    out = [(i, id2tok[i], dists[i]) for i in ind]\n", "    return out\n", "\n", "dmat = get_distance_matrix(wordvecs, 'cosine')\n", "for word in tokens:\n", "    print(word, [t[1] for t in get_k_similar_words(word, dmat)], \"\\n\")"]}, {"cell_type": "markdown", "id": "e1d8137d", "metadata": {"ExecuteTime": {"end_time": "2023-08-07T00:38:47.402752Z", "start_time": "2023-08-07T00:38:47.384725Z"}, "deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "639c2eb3f9df4bd7244c8a482d492350", "grade": false, "grade_id": "cell-5e5ce486e8860c29", "locked": true, "schema_version": 3, "solution": false, "task": false}, "id": "e1d8137d"}, "source": ["**PREGUNTAS:**\n", "* \u00bfCu\u00e1l es la implicaci\u00f3n del overfitting en modelos como Word2Vec?\n", "* \u00bfQu\u00e9 tan bien encontr\u00f3 palabras cercanas su modelo Word2Vec? \u00bfPodr\u00eda mejorar? \u00bfC\u00f3mo podr\u00eda mejorar?\n", "* A grandes rasgos, \u00bfcu\u00e1l es la diferencia entre Word2Vec y BERT?"]}, {"cell_type": "markdown", "id": "de1f5579", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "2a9b67df22196d0cba2dfbdcb310efc3", "grade": false, "grade_id": "cell-b8252dd53c19ab29", "locked": true, "schema_version": 3, "solution": false, "task": false}, "id": "de1f5579"}, "source": ["## Parte 2 - Encoder - Decoder\n", "\n", "**Cr\u00e9ditos:** La segunda parte de este laboratorio est\u00e1 tomado y basado en uno de los repositorios de Ben Trevett\n", "\n", "En esta ocasi\u00f3n vamos a centrarnos en una arquitectura Sequence to Sequence (Seq2Seq), entonces estaremos desarrollando un modelo que nos ayude a traducir de alem\u00e1n a ingl\u00e9s. Tomaremos como base el paper [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215). Recuerden que a pesar que esto es para frases/oraciones, los conceptos pueden ser aplicados para otras arquitecturas similares.\n", "\n", "**IMPORTANTE:** Recuerden usar virtual enviroments debido a que estaremos usando versiones viejas de la librer\u00edas. \u00bfPor qu\u00e9? Las librer\u00edas eran un poco m\u00e1s expl\u00edcitas que sus versiones m\u00e1s recientes. A continuaci\u00f3n se dejan los comandos para la instalaci\u00f3n de las m\u00e1s importantes\n", "\n", "```\n", "pip install -U torch==1.9.0+cu111 -f  https://download.pytorch.org/whl/cu111/torch_stable.html\n", "pip install -U torchtext==0.10.0\n", "```\n", "\n", "El primer comando instalar\u00e1 la librer\u00eda de PyTorch con CUDA 11.1\n", "El segundo, instala TorchText en una versi\u00f3n donde la formulaci\u00f3n del vocabulario para training, test y validation era m\u00e1s claro (esta es la principal por la que estamos usando esta versiones).\n", "\n", "\n", "### Introducci\u00f3n\n", "Los modelos m\u00e1s comunes seq2seq son los modelos *encoder-decoder*, los cuales usan una RNN para encodear el input y llevarlo a un solo vector. En este laboratorio nos estaremos refiriendo a dicho vector como *vector contexto*. Pensemos sobre el vector contexto como un ser abstracto que representa una frase completa. Este vector es luego decodeado por una segunda RNN, que aprende a generar la frase target (output) deseada al generar palabra por palabra.\n", "\n", "\n", "Consideren la siguiente ilustraci\u00f3n para representar el proceso que estaremos realizando\n", "\n", "<img src=\"https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/49df8404d938a6edbf729876405558cc2c2b3013/assets/seq2seq1.png\" alt=\"Seq2Seq\" />\n", "\n", "*Cr\u00e9dito de imagen al autor, imagen tomada de \"Sequence to Sequence Learning with Neural Networks\" de Ben Trevett*\n", "\n", "Noten como la frase de input \"guten morgen\", se pasa a trav\u00e9s de una capa de embedding (cuadros amarillos) y luego entra en los encodeadores (cuadros verdes). En esta ocasi\u00f3n agregamos un token de \"start of sequence\" (`<sos>`) al inicio de la frase, adem\u00e1s de un token de \"end of sequence\" (`<eos>`) al final de la oraci\u00f3n. Vean como en cada paso, la entrada del encoder RNN es tanto la representaci\u00f3n embedding $e$ de la palabra actual $e(x_t)$, as\u00ed como el estado oculto del paso anterior $h_{t-1}$, y el encoder genera un nuevo hidden state $h_t$. Entonces, podemos pensar en el hidden state como una representaci\u00f3n vectorial de la oraci\u00f3n hasta ese momento. La RNN se puede representar como una funci\u00f3n de tanto $e(x_t)$ y $h_{t-1}$\n", "\n", "$$h_t = \\text{EncoderRNN}(e(x_t), h_{t-1})$$\n", "\n", "Por favor noten que estamos usando el termino RNN de forma general en este contexto, puede ser cualquier arquitectura como LSTM o GRU.\n", "\n", "Entonces estaremos trabajando con una secuencia como $X = \\{x_1, x_2, ..., x_T\\}$, donde $x_1 = <sos>$, $x_2 = guten$, y as\u00ed consecutivamente. El hidden state inicial $h_0$ es usualmente iniciado con ceros o con alg\u00fan parametro pre-aprendido.\n", "\n", "Una vez la palabra final $X_T$ ha pasado en la RNN a trav\u00e9s de la embedding layer, usamos el hidden state final $h_T$ como vector de contexto. Es decir, $h_T = z$. El cual ser\u00e1 la representaci\u00f3n vectorial de toda la oraci\u00f3n.\n", "\n", "Ahora que tenemos nuestro vector de contexto $z$, podemos empezar a decodear para obtener la oraci\u00f3n target, \"good morning\". De nuevo, agregamos los tokens de inicio y fin de la secuencia de nuestra oraci\u00f3n target. En cada paso, el input al decoder RNN (cuadros azules de la imagen) es la versi\u00f3n embedding $d$ de la palabra actual $d(y_t)$ as\u00ed como tambi\u00e9n el hidden state del paso previo $s_{t-1}$m donde el hidden state del decoder incial $s_0$ es el vector de contexto $s_0 = z = h_T$, es decir, el hidden state decoder es el \u00faltimo hidden state encoder. Por ende, simlar al encoder, podemos representarlo como:\n", "\n", "$$s_t = \\text{DecoderRNN}(d(y_t), s_{t-1})$$\n", "\n", "A pesar que el input embeeding layer $e$ y el target embedding layer $d$ est\u00e1n representados como cuadros amarillos en la imagen, como dijimos en clase, estas son dos embedding layers diferentes con sus propios parametros.\n", "\n", "En el decoder, necestamos ir del hidden state a la palabra actual, por ello en cada paso usamos $s_t$ para predecir (a traves de pasarlo en una layer lineal, mostrada como cuadros morados) lo que se cree que es la siguiente palabra en la secuencia $\\hat{y}_t$\n", "\n", "$$\\hat{y}_t = f(s_t)$$\n", "\n", "Las palabras en el decoder son siempre generadas una despu\u00e9s de la otra, con una por paso. Siempre usamos `<sos>` para el primer input del decodr $y_1$ y algumas veces usamos la palabra predicha por nuestro decoder, $\\hat{y}_{t-1}$. Que, como mencionamos en clase, se le llama *teacher forcing*.\n", "\n", "Cuando estamos entrenando o probando nuestro modelo, siempre sabemso cuantas palabras hay en nuestra secuencia target, entonces nos detenemos de generar palabras una vez alcanzamos esa cantidad. Durante las fases de inferencia (uso del modelo en la \"vida real\") seguimos generando palabras hasta que el modelo genere un token `<eos>` o despu\u00e9s de una cierta cantidad de palabras dada. (Esto tambien lo mencionamoos en clase, es solo para refrescar los conceptos)\n", "\n", "Una vez tengamos nuestra secuencia target predicha $\\hat{Y} = \\{ \\hat{y}_1, \\hat{y}_2, ..., \\hat{y}_T \\}$, la comparamos contra nuestra secuencia target real. $Y = \\{ y_1, y_2, ..., y_T \\}$, para calcular la perdida. Usamos esta p\u00e9rdida para actualizar los par\u00e1metros del modelo, como bien hemos hecho en otras ocasiones.\n", "\n", "### Preparaci\u00f3n de Data\n", "\n", "Es momento de ponernos a manos a la obra. Estaremos programando nuestro modelo usando PyTorch y usando torchtext para ayudarnos a hacer todo el pre-procesamiento necesario. Ahora usaremos spaCy para ayudarnos en la tokenizaci\u00f3n de los datos"]}, {"cell_type": "code", "execution_count": 4, "id": "6b5246f2", "metadata": {"ExecuteTime": {"end_time": "2023-08-07T12:10:04.622634Z", "start_time": "2023-08-07T12:09:55.185815Z"}, "id": "6b5246f2"}, "outputs": [], "source": ["import torch\n", "import torch.nn as nn\n", "import torch.optim as optim\n", "\n", "from torchtext.datasets import Multi30k\n", "\n", "train_url = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n", "val_url = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n", "test_url = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/mmt16_task1_test.tar.gz\"\n", "\n", "# Update the URLs in the Multi30k module\n", "Multi30k.urls = (train_url, val_url, test_url)\n", "\n", "from torchtext.data import Field, BucketIterator\n", "\n", "import spacy\n", "import numpy as np\n", "\n", "import random\n", "import math\n", "import time"]}, {"cell_type": "markdown", "id": "bb5f19f9", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "cb193611b74e155202745762d0e05a22", "grade": false, "grade_id": "cell-960a94af6dbde6f6", "locked": true, "schema_version": 3, "solution": false, "task": false}, "id": "bb5f19f9"}, "source": ["Colocamos las semillas para tener resultados consistentes."]}, {"cell_type": "code", "execution_count": 5, "id": "e2182c4e", "metadata": {"ExecuteTime": {"end_time": "2023-08-07T12:10:12.966554Z", "start_time": "2023-08-07T12:10:12.934552Z"}, "id": "e2182c4e"}, "outputs": [], "source": ["SEED = 1234\n", "\n", "random.seed(SEED)\n", "np.random.seed(SEED)\n", "torch.manual_seed(SEED)\n", "torch.cuda.manual_seed(SEED)\n", "torch.backends.cudnn.deterministic = True"]}, {"cell_type": "markdown", "id": "85333315", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "32e4c72dc8e544489e4bb9de559fc363", "grade": false, "grade_id": "cell-1a4d13aff19cbbc7", "locked": true, "schema_version": 3, "solution": false, "task": false}, "id": "85333315"}, "source": ["Ahora, necesitamos crear un tokenizador. Un tokenizador es una funci\u00f3n que ayudar\u00e1 a convertir un string de alguna frase u oraci\u00f3n en una lista de tokens individuales. Consideren que en una frase como \"good morning!\" se tienen tres tokens, siendo cada uno \"good\", \"morning\" y \"!\", noten que a pesar que el signo de admiracion no se considera una palabra, s\u00ed se considera como un token.\n", "\n", "Para la creaci\u00f3n de nuestro tokenizador nos apoyaremos en spaCy, en este caso necesitamos los paquetes de aleman e ingl\u00e9s (se nombran abajo).\n", "\n", "Para instalar spaCy necesitar\u00e1n ejecutar en la cmd\n", "```\n", "pip install spacy\n", "python -m spacy download en_core_web_sm\n", "python -m spacy download de_core_news_sm\n", "```\n", "\n", "**IMPORTANTE:** Recuerden usar virtual environments de Python, debido a que este laboratorio usa algunas librer\u00edas deprecadas, que como se explic\u00f3 previamente, se hizo de este modo para ser m\u00e1s expl\u00edcito el aprendizaje.\n", "\n", "Regresando al tema del tokenizer, primero cargaremos las dos versiones para los diferentes idiomas con los que estamos trabajando.\n", "\n", "Despues, crearemos unas funciones de tokenizaci\u00f3n. Estas pueden ser pasadas a TorchText y tomar\u00e1n una oraci\u00f3n y regresara la oraci\u00f3n como una lista de tokens.\n", "\n", "Cabe la pena mencionar que en el paper que estamos tomando de base, ellos encontrar\u00f3n util el revertir el orden del input dado que se cree que introduc\u00eda varias dependencias a corto plazo en los datos que facilitan mucho el problema de optimizaci\u00f3n.\n", "\n", "M\u00e1s adelante, usaremos `Field` (que actualmente est\u00e1 deprecado :( ) para manejar como la data deber\u00eda ser procesada. Despu\u00e9s, seteamos el parametro `tokenize` como funci\u00f3n para cada caso. El aleman ser\u00e1 el `SRC` y el ingl\u00e9s ser\u00e1 el `TRG`. Adem\u00e1s tambi\u00e9n se agrega el token para inicio y fin de la secuencia, adem\u00e1s que convertir\u00e1 todo en lowercase."]}, {"cell_type": "code", "execution_count": 6, "id": "d4bc4d31", "metadata": {"ExecuteTime": {"end_time": "2023-08-07T12:10:15.907822Z", "start_time": "2023-08-07T12:10:14.202661Z"}, "id": "d4bc4d31"}, "outputs": [], "source": ["spacy_de = spacy.load('de_core_news_sm')\n", "spacy_en = spacy.load('en_core_web_sm')"]}, {"cell_type": "code", "execution_count": 7, "id": "16e7543a", "metadata": {"ExecuteTime": {"end_time": "2023-08-07T12:10:16.426717Z", "start_time": "2023-08-07T12:10:16.410680Z"}, "id": "16e7543a"}, "outputs": [], "source": ["def tokenize_de(text):\n", "    \"\"\"\n", "    Tokenizes German text from a string into a list of strings (tokens) and reverses it\n", "    \"\"\"\n", "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n", "\n", "def tokenize_en(text):\n", "    \"\"\"\n", "    Tokenizes English text from a string into a list of strings (tokens)\n", "    \"\"\"\n", "    return [tok.text for tok in spacy_en.tokenizer(text)]"]}, {"cell_type": "code", "execution_count": 8, "id": "1227f75a", "metadata": {"ExecuteTime": {"end_time": "2023-08-07T12:10:16.775400Z", "start_time": "2023-08-07T12:10:16.767392Z"}, "id": "1227f75a"}, "outputs": [], "source": ["SRC = Field(tokenize = tokenize_de,\n", "            init_token = '<sos>',\n", "            eos_token = '<eos>',\n", "            lower = True)\n", "\n", "TRG = Field(tokenize = tokenize_en,\n", "            init_token = '<sos>',\n", "            eos_token = '<eos>',\n", "            lower = True)"]}, {"cell_type": "markdown", "id": "d48fd2f4", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "e4c7c0410bb12c03fd3a415d086f6aa1", "grade": false, "grade_id": "cell-f059a0654bba3e20", "locked": true, "schema_version": 3, "solution": false, "task": false}, "id": "d48fd2f4"}, "source": ["Ahora, debemos descargar el dataset. Para este caso estaremos usando el dataset llamado Multi30k. Este tiene aproximadamente 30K frases en ingl\u00e9s, aleman y franc\u00e9s, cada uno tiene alrededor de 12 palabras por frase.\n", "\n", "Adem\u00e1s noten que `exts` especifica cual lenguage se debe usar como source y target, y `fields` da cuales campos usar para el source y target."]}, {"cell_type": "code", "execution_count": 10, "id": "5c2f3811", "metadata": {"ExecuteTime": {"end_time": "2023-08-07T12:10:21.558532Z", "start_time": "2023-08-07T12:10:17.137695Z"}, "id": "5c2f3811"}, "outputs": [], "source": ["train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),\n", "                                                    fields = (SRC, TRG))\n", "\n"]}, {"cell_type": "code", "execution_count": 11, "id": "9daf8a6a", "metadata": {"ExecuteTime": {"end_time": "2023-08-07T12:10:21.990868Z", "start_time": "2023-08-07T12:10:21.982854Z"}, "colab": {"base_uri": "https://localhost:8080/"}, "id": "9daf8a6a", "outputId": "9d60913e-4002-464f-c201-79dd46e3b919"}, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["Numero de observaciones de training: 29000\n", "Numero de observaciones en validation: 1014\n", "Numero de observaciones en test: 1000\n"]}], "source": ["print(f\"Numero de observaciones de training: {len(train_data.examples)}\")\n", "print(f\"Numero de observaciones en validation: {len(valid_data.examples)}\")\n", "print(f\"Numero de observaciones en test: {len(test_data.examples)}\")"]}, {"cell_type": "code", "execution_count": 12, "id": "56422a53", "metadata": {"ExecuteTime": {"end_time": "2023-08-07T12:10:23.001453Z", "start_time": "2023-08-07T12:10:22.753770Z"}, "id": "56422a53"}, "outputs": [], "source": ["SRC.build_vocab(train_data, min_freq = 2)\n", "TRG.build_vocab(train_data, min_freq = 2)"]}, {"cell_type": "code", "execution_count": 13, "id": "af8e7be9", "metadata": {"ExecuteTime": {"end_time": "2023-08-07T12:10:22.369708Z", "start_time": "2023-08-07T12:10:22.353942Z"}, "colab": {"base_uri": "https://localhost:8080/"}, "id": "af8e7be9", "outputId": "284e3f7a-62ce-4931-dbf6-a3fe3fd02e7f"}, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["{'src': ['.', 'b\u00fcsche', 'vieler', 'n\u00e4he', 'der', 'in', 'freien', 'im', 'sind', 'm\u00e4nner', 'wei\u00dfe', 'junge', 'zwei'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}\n"]}], "source": ["print(vars(train_data.examples[0]))"]}, {"cell_type": "markdown", "id": "f09e20d7", "metadata": {"id": "f09e20d7"}, "source": ["Observen como el punto est\u00e1 al comienzo de la oraci\u00f3n en alem\u00e1n (src), por lo que parece que la oraci\u00f3n se invirti\u00f3 correctamente.\n", "\n", "Ahora, construiremos el vocabulario para los idiomas de source y de target. El vocabulario se utiliza para asociar cada token \u00fanico con un \u00edndice (un n\u00famero entero). Los vocabularios de los idiomas de origen y de destino son distintos.\n", "\n", "Usando el argumento `min_freq`, solo permitimos que aparezcan en nuestro vocabulario tokens que aparecen al menos 2 veces. Los tokens que aparecen solo una vez se convierten en un token desconocido `<unk>`.\n", "\n", "Es importante tener en cuenta que nuestro vocabulario solo debe construirse a partir del conjunto de entrenamiento y no del conjunto de validaci\u00f3n/test. Esto evita la \"fuga de informaci\u00f3n\" en nuestro modelo, d\u00e1ndonos puntajes de validaci\u00f3n/prueba inflados artificialmente."]}, {"cell_type": "code", "execution_count": 14, "id": "0b865dee", "metadata": {"ExecuteTime": {"end_time": "2023-08-07T12:10:23.312139Z", "start_time": "2023-08-07T12:10:23.299580Z"}, "colab": {"base_uri": "https://localhost:8080/"}, "id": "0b865dee", "outputId": "bf1785ae-d08d-4b82-ec94-e2d877587791"}, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["Unique tokens in source (de) vocabulary: 7853\n", "Unique tokens in target (en) vocabulary: 5893\n"]}], "source": ["print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n", "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"]}, {"cell_type": "markdown", "id": "3686bcfc", "metadata": {"id": "3686bcfc"}, "source": ["El paso final de preparar los datos es crear los iteradores. Estos se pueden iterar para devolver un lote de datos que tendr\u00e1 un atributo `src` (los tensores de PyTorch\n", "que contienen un lote de oraciones de origen numeradas) y un atributo `trg` (los tensores de PyTorch que contienen un batch de oraciones de destino numeradas).\n", "\"Numericalized\" es solo una forma elegante de decir que se han convertido de una secuencia de tokens legibles a una secuencia de \u00edndices correspondientes, usando el vocabulario.\n", "\n", "Tambi\u00e9n necesitamos definir un dispositivo `torch.device`. Esto se usa para indicarle a torchText que coloque o no los tensores en la GPU.\n", "Usamos la funci\u00f3n `torch.cuda.is_available()`, que devolver\u00e1 True si se detecta una GPU en nuestra computadora. Pasamos este dispositivo al iterador.\n", "\n", "Cuando obtenemos un lote de ejemplos usando un iterador, debemos asegurarnos de que todas las oraciones de origen tengan la misma longitud,\n", "al igual que las oraciones de destino. \u00a1Afortunadamente, los iteradores de torchText manejan esto por nosotros!\n", "\n", "Usamos un `BucketIterator` en lugar del `Iterador` est\u00e1ndar, ya que crea lotes de tal manera que minimiza la cantidad de padding en las oraciones de origen y de destino."]}, {"cell_type": "code", "execution_count": 15, "id": "c6fbb387", "metadata": {"ExecuteTime": {"end_time": "2023-08-07T12:10:23.716604Z", "start_time": "2023-08-07T12:10:23.695697Z"}, "colab": {"base_uri": "https://localhost:8080/"}, "id": "c6fbb387", "outputId": "8abc65e0-5177-42e7-d433-868442b09aac"}, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["cpu\n"]}], "source": ["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n", "print(device)"]}, {"cell_type": "code", "execution_count": 16, "id": "fa44707d", "metadata": {"ExecuteTime": {"end_time": "2023-08-07T12:10:29.661585Z", "start_time": "2023-08-07T12:10:29.630173Z"}, "id": "fa44707d"}, "outputs": [], "source": ["BATCH_SIZE = 128\n", "\n", "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n", "    (train_data, valid_data, test_data),\n", "    batch_size = BATCH_SIZE,\n", "    device = device)"]}, {"cell_type": "markdown", "id": "725df266", "metadata": {"id": "725df266"}, "source": ["### Construyendo el Modelo Seq2Seq\n", "Vamos a definir nuestro modelo en tres partes, el encoder, el decoder y el modelo Seq2Seq. Este ultimo encapsular\u00e1 el proceso y transferencia entre los primeros dos.\n", "\n", "#### Encoder\n", "Primero, el encoder, es un LSTM de 2 capas. El paper que estamos implementando usa un LSTM de 4 capas, pero en favor del tiempo de entrenamiento lo reducimos a 2 capas.\n", "El concepto de RNN multicapa es f\u00e1cil de expandir de 2 a 4 capas.\n", "\n", "Para un RNN multicapa, la oraci\u00f3n de entrada, $X$, despu\u00e9s de ser embeddida va a la primera capa (inferior) del RNN y los estados ocultos, $H=\\{h_1, h_2, ..., h_T\\}$ ,\n", "la salida de esta capa se utiliza como entrada a la RNN en la capa superior. As\u00ed, representando cada capa con un super\u00edndice, los hidden states en la primera capa vienen dados por:\n", "\n", "$$h_t^1 = \\text{EncoderRNN}^1(e(x_t), h_{t-1}^1)$$\n", "\n", "Las hidden states en la segunda layer son dadas por:\n", "\n", "$$h_t^2 = \\text{EncoderRNN}^2(h_t^1, h_{t-1}^2)$$\n", "\n", "El uso de un RNN multicapa tambi\u00e9n significa que tambi\u00e9n necesitaremos un hidden state inicial como entrada por capa, $h_0^l$, y tambi\u00e9n generaremos un vector de contexto por capa, $z^l$.\n", "\n", "Si desean repasar un poco sobre LSTM pueden consultar este [enlance] (https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n", "Para este laboratorio, es suficiente que recuerden que lo que necesitamos saber es los LSTM, en lugar de simplemente tomar un estado oculto y devolver un nuevo estado oculto por paso de tiempo,\n", "tambi\u00e9n toman y devuelven un *estado de celda*, $c_t$, por paso de tiempo.\n", "\n", "$$\\begin{align*}\n", "h_t &= \\text{RNN}(e(x_t), h_{t-1})\\\\\n", "(h_t, c_t) &= \\text{LSTM}(e(x_t), h_{t-1}, c_{t-1})\n", "\\end{align*}$$\n", "\n", "Podemos pensar en $c_t$ como otro tipo de hidden state. Similar a $h_0^l$, $c_0^l$ se inicializar\u00e1 en un tensor de ceros.\n", "Adem\u00e1s, nuestro vector de contexto ahora ser\u00e1 tanto el hidden state final como el estado de celda final, es decir, $z^l = (h_T^l, c_T^l)$.\n", "\n", "Al extender nuestras ecuaciones multicapa a LSTM, obtenemos:\n", "\n", "$$\\begin{align*}\n", "(h_t^1, c_t^1) &= \\text{EncoderLSTM}^1(e(x_t), (h_{t-1}^1, c_{t-1}^1))\\\\\n", "(h_t^2, c_t^2) &= \\text{EncoderLSTM}^2(h_t^1, (h_{t-1}^2, c_{t-1}^2))\n", "\\end{align*}$$\n", "\n", "\n", "Observen c\u00f3mo solo nuestro hidden state de la primera capa se pasa como entrada a la segunda capa, y no el estado de la celda.\n", "\n", "As\u00ed que nuestro codificador se parece a esto:\n", "\n", "IMAGEN\n", "\n", "Creamos esto en el c\u00f3digo creando un m\u00f3dulo `Encoder`, que requiere que heredemos de `torch.nn.Module` y usemos `super().__init__()` como un c\u00f3digo repetitivo.\n", "El codificador toma los siguientes argumentos:\n", "- `input_dim` es el tama\u00f1o/dimensionalidad de los vectores one-hot que se ingresar\u00e1n al codificador. Esto es igual al tama\u00f1o del vocabulario de entrada (fuente).\n", "- `emb_dim` es la dimensionalidad de la capa de embedding. Esta capa convierte los vectores one-hot en vectores densos con dimensiones `emb_dim`.\n", "- `hid_dim` es la dimensionalidad de los estados ocultos y de celda.\n", "- `n_layers` es el n\u00famero de capas en el RNN.\n", "- `dropout` es la cantidad de abandono a utilizar. Este es un par\u00e1metro de regularizaci\u00f3n para evitar el overfitting.\n", "Consulte [aqui] (https://www.coursera.org/lecture/deep-neural-network/understanding-dropout-YaGbR) para obtener m\u00e1s detalles sobre dropout.\n", "\n", "\n", "No vamos a discutir la capa de embedding en detalle durante aqui pues ya lo hicimos previamente. Todo lo que necesitamos saber es que hay un paso antes de que las palabras\n", "(t\u00e9cnicamente, los \u00edndices de las palabras) pasen al RNN, donde las palabras se transforman en vectores. Para leer m\u00e1s sobre embedding de palabras,\n", "consulten estos art\u00edculos: [1](https://monkeylearn.com/blog/word-embeddings-transform-text-numbers/), [2](http://p.migdal.pl /2017/01/06/rey-hombre-mujer-reina-por qu\u00e9.html), [3](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/ ), [4](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/).\n", "\n", "La capa de embedding se crea usando `nn.Embedding`, el LSTM con `nn.LSTM` y una capa de dropout con `nn.Dropout`.\n", "Consulten la [documentaci\u00f3n de PyTorch ] (https://pytorch.org/docs/stable/nn.html) para obtener m\u00e1s informaci\u00f3n al respecto.\n", "\n", "Una cosa a tener en cuenta es que el argumento `dropout` para el LSTM es cu\u00e1nto dropout aplicar entre las capas de un RNN multicapa,\n", "es decir, entre la salida de estados ocultos de la capa $l$ y esos mismos estados ocultos que se utilizan para el entrada de la capa $l+1$.\n", "\n", "En el m\u00e9todo `forward`, pasamos la oraci\u00f3n fuente, $X$, que se convierte en vectores densos usando la capa `embedding`, y luego se aplica el dropout.\n", "Estos embedding luego se pasan a la RNN. A medida que pasamos una secuencia completa a la RNN, \u00a1autom\u00e1ticamente har\u00e1 el c\u00e1lculo recurrente de los estados\n", "ocultos en toda la secuencia por nosotros! Tenga en cuenta que no pasamos un estado inicial oculto o de celda al RNN.\n", "Esto se debe a que, como se indica en la [documentaci\u00f3n](https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM), si no se pasa ning\u00fan estado de celda/oculto a la RNN, crea autom\u00e1ticamente un estado inicial de celda/oculto como un tensor de ceros.\n", "\n", "El RNN devuelve: `outputs` (el hidden state de la capa superior para cada paso de tiempo), `hidden` (el hidden state final para cada capa, $h_T$,\n", "apiladas una encima de la otra) y `cell` (la estado de celda final para cada capa, $c_T$, apilados uno encima del otro).\n", "\n", "Como solo necesitamos los hidden state y de celda finales (para hacer nuestro vector de contexto), `forward` solo devuelve `hidden` y `cell`.\n", "\n", "Los tama\u00f1os de cada uno de los tensores se dejan como comentarios en el c\u00f3digo. En esta implementaci\u00f3n, `n_directions` siempre ser\u00e1 1, sin embargo,\n", "tengan en cuenta que los RNN bidireccionales (cubiertos en el tutorial 3) tendr\u00e1n `n_directions` como 2."]}, {"cell_type": "code", "execution_count": 17, "id": "1b79c7b4", "metadata": {"ExecuteTime": {"end_time": "2023-08-07T12:10:30.685603Z", "start_time": "2023-08-07T12:10:30.659165Z"}, "deletable": false, "nbgrader": {"cell_type": "code", "checksum": "24a7fe486fef7641af76e0308c8553e5", "grade": false, "grade_id": "cell-819cfe4960d74aaf", "locked": false, "schema_version": 3, "solution": true, "task": false}, "id": "1b79c7b4"}, "outputs": [], "source": ["class Encoder(nn.Module):\n", "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n", "        super().__init__()\n", "\n", "        self.hid_dim = hid_dim\n", "        self.n_layers = n_layers\n", "\n", "        # Aprox 1 linea para\n", "        # self.embedding =\n", "        self.embedding = nn.Embedding(input_dim, emb_dim)\n", "        # YOUR CODE HERE\n", "        #raise NotImplementedError()\n", "\n", "        # Aprox 1 linea para\n", "        # self.rnn =\n", "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n", "        # YOUR CODE HERE\n", "        #raise NotImplementedError()\n", "\n", "        self.dropout = nn.Dropout(dropout)\n", "\n", "    def forward(self, src):\n", "\n", "        #src = [src len, batch size]\n", "\n", "        embedded = self.dropout(self.embedding(src))\n", "\n", "        #embedded = [src len, batch size, emb dim]\n", "\n", "        outputs, (hidden, cell) = self.rnn(embedded)\n", "\n", "        #outputs = [src len, batch size, hid dim * n directions]\n", "        #hidden = [n layers * n directions, batch size, hid dim]\n", "        #cell = [n layers * n directions, batch size, hid dim]\n", "\n", "        #outputs are always from the top hidden layer\n", "\n", "        return hidden, cell"]}, {"cell_type": "markdown", "id": "e85acaa4", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "d9c175e73457d779955f7a23deb52244", "grade": false, "grade_id": "cell-5f5382d652012307", "locked": true, "schema_version": 3, "solution": false, "task": false}, "id": "e85acaa4"}, "source": ["#### Decoder\n", "Ahora pasaremos a construir el decoder, el cual tambi\u00e9n ser\u00e1 una 2-layer (4 en el paper) LSTM.\n", "\n", "![](https://github.com/ElrohirGT/DeepLearning-Lab4/blob/main/assets/seq2seq3.png?raw=1)\n", "\n", "\n", "La clase `Decoder` hace un solo paso de decodificaci\u00f3n, es decir, genera un solo token por paso. La primera capa recibir\u00e1 un hidden state y de celda del paso de tiempo anterior,\n", "$(s_{t-1}^1, c_{t-1}^1)$, y lo alimenta a trav\u00e9s del LSTM con el token incrustado actual, $y_t$, para producir un nuevo hidden state y de celda, $(s_t ^1, c_t^1)$.\n", "Las capas subsiguientes usar\u00e1n el estado oculto de la capa inferior, $s_t^{l-1}$, y los estados ocultos y de celda anteriores de su capa, $(s_{t-1}^l, c_{t-1) }^l)$.\n", "Esto proporciona ecuaciones muy similares a las del codificador.\n", "\n", "$$\\begin{align*}\n", "(s_t^1, c_t^1) = \\text{DecoderLSTM}^1(d(y_t), (s_{t-1}^1, c_{t-1}^1))\\\\\n", "(s_t^2, c_t^2) = \\text{DecoderLSTM}^2(s_t^1, (s_{t-1}^2, c_{t-1}^2))\n", "\\end{align*}$$\n", "\n", "\n", "Recuerde que los estados iniciales ocultos y de celda de nuestro decoder son nuestros vectores de contexto, que son los estados finales ocultos y de celda de nuestro decoder de la misma capa,\n", "es decir, $(s_0^l,c_0^l)=z^l=(h_T^l,c_T^l)$.\n", "\n", "Luego pasamos el hidden state desde la capa superior del RNN, $s_t^L$, a trav\u00e9s de una capa lineal, $f$, para hacer una predicci\u00f3n de cu\u00e1l ser\u00e1 el siguiente token en la secuencia de destino (salida).\n", "deber\u00eda ser, $\\hat{y}_{t+1}$.\n", "\n", "$$\\sombrero{y}_{t+1} = f(s_t^L)$$\n", "\n", "Los argumentos y la inicializaci\u00f3n son similares a la clase `Encoder`, excepto que ahora tenemos un `output_dim` que es el tama\u00f1o del vocabulario para la salida/objetivo.\n", "Tambi\u00e9n est\u00e1 la adici\u00f3n de la capa 'Lineal', utilizada para hacer las predicciones desde el hidden state de la capa superior.\n", "\n", "Dentro del m\u00e9todo `forward`, aceptamos un batch de tokens de entrada, hidden state anteriores y estados de celda anteriores. Como solo estamos decodificando un token a la vez,\n", "los tokens de entrada siempre tendr\u00e1n una longitud de secuencia de 1. \"Aflojamos\" los tokens de entrada para agregar una dimensi\u00f3n de longitud de oraci\u00f3n de 1. Luego, de forma similar al encoder,\n", "pasamos a trav\u00e9s de una capa de embedding y aplicamos dropout. Este batch de tokens embeddidos luego se pasa al RNN con los estados ocultos y de celda anteriores.\n", "Esto produce una \"salida\" (hidden state de la capa superior de la RNN), un nuevo \"hidden state\" (uno para cada capa, apilados uno encima del otro) y una nueva \"celda\".\n", "estado (tambi\u00e9n uno por capa, apilados uno encima del otro). Luego pasamos la `salida` (despu\u00e9s de deshacernos de la dimensi\u00f3n de longitud de la oraci\u00f3n) a trav\u00e9s de la capa lineal para recibir nuestra\n", "`predicci\u00f3n`. Luego devolvemos la `predicci\u00f3n`, el nuevo hidden state y el nuevo estado `celular`.\n", "\n", "**Nota**: como siempre tenemos una longitud de secuencia de 1, podr\u00edamos usar `nn.LSTMCell`, en lugar de `nn.LSTM`, ya que est\u00e1 dise\u00f1ado para manejar un lote de entradas que no son\n", "necesariamente en una secuencia. `nn.LSTMCell` es solo una sola celda y `nn.LSTM` es un envoltorio alrededor de m\u00faltiples celdas potenciales. Usando `nn.LSTMCell` en este caso\n", "significar\u00eda que no tenemos que `descomprimir` para agregar una dimensi\u00f3n de longitud de secuencia falsa, pero necesitar\u00edamos un `nn.LSTMCell` por capa en el decoder y para asegurar que cada `nn.LSTMCell`\n", "recibe el hidden state inicial correcto del codificador. Todo esto hace que el c\u00f3digo sea menos conciso, de ah\u00ed la decisi\u00f3n de seguir con el `nn.LSTM` regular."]}, {"cell_type": "code", "execution_count": 18, "id": "caa53fcf", "metadata": {"ExecuteTime": {"end_time": "2023-08-07T12:10:33.673863Z", "start_time": "2023-08-07T12:10:33.642635Z"}, "deletable": false, "nbgrader": {"cell_type": "code", "checksum": "9062a639f1c3bd604869ed020a65ea7e", "grade": false, "grade_id": "cell-84131f43444e74fa", "locked": false, "schema_version": 3, "solution": true, "task": false}, "id": "caa53fcf"}, "outputs": [], "source": ["class Decoder(nn.Module):\n", "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n", "        super().__init__()\n", "\n", "        # Aprox 3 lineas para\n", "        self.output_dim = output_dim\n", "        self.hid_dim = hid_dim\n", "        self.n_layers = n_layers\n", "        # YOUR CODE HERE\n", "        # raise NotImplementedError()\n", "\n", "        # Aprox 1 linea para\n", "        # self.embedding =\n", "        self.embedding = nn.Embedding(output_dim, emb_dim)\n", "        # YOUR CODE HERE\n", "        # raise NotImplementedError()\n", "\n", "        # Aprox 1 linea para\n", "        # self.rnn =\n", "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n", "        # YOUR CODE HERE\n", "        # raise NotImplementedError()\n", "\n", "        self.fc_out = nn.Linear(hid_dim, output_dim)\n", "\n", "        self.dropout = nn.Dropout(dropout)\n", "\n", "    def forward(self, input, hidden, cell):\n", "\n", "        #input = [batch size]\n", "        #hidden = [n layers * n directions, batch size, hid dim]\n", "        #cell = [n layers * n directions, batch size, hid dim]\n", "\n", "        #n directions in the decoder will both always be 1, therefore:\n", "        #hidden = [n layers, batch size, hid dim]\n", "        #context = [n layers, batch size, hid dim]\n", "\n", "        input = input.unsqueeze(0)\n", "\n", "        #input = [1, batch size]\n", "\n", "        embedded = self.dropout(self.embedding(input))\n", "\n", "        #embedded = [1, batch size, emb dim]\n", "\n", "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n", "\n", "        #output = [seq len, batch size, hid dim * n directions]\n", "        #hidden = [n layers * n directions, batch size, hid dim]\n", "        #cell = [n layers * n directions, batch size, hid dim]\n", "\n", "        #seq len and n directions will always be 1 in the decoder, therefore:\n", "        #output = [1, batch size, hid dim]\n", "        #hidden = [n layers, batch size, hid dim]\n", "        #cell = [n layers, batch size, hid dim]\n", "\n", "        prediction = self.fc_out(output.squeeze(0))\n", "\n", "        #prediction = [batch size, output dim]\n", "\n", "        return prediction, hidden, cell"]}, {"cell_type": "markdown", "id": "c2746420", "metadata": {"id": "c2746420"}, "source": ["### Seq2Seq\n", "\n", "\n", "Para la parte final de la implementaci\u00f3n, implementaremos el modelo seq2seq. Esto manejar\u00e1:\n", "- recibir la oraci\u00f3n de entrada/fuente\n", "- usar el encoder para producir los vectores de contexto\n", "- usar el decoder para producir la salida predicha/oraci\u00f3n objetivo\n", "\n", "Nuestro modelo completo se ver\u00e1 as\u00ed:\n", "\n", "![](https://github.com/ElrohirGT/DeepLearning-Lab4/blob/main/activos/seq2seq4.png?raw=1)\n", "\n", "El modelo `Seq2Seq` incluye un `Encoder`, un `Decoder` y un `dispositivo` (usado para colocar tensores en la GPU, si existe).\n", "\n", "Para esta implementaci\u00f3n, debemos asegurarnos de que el n\u00famero de capas y las dimensiones ocultas (y de celda) sean iguales en el 'Encoder' y 'Decoder'.\n", "Este no es siempre el caso, no necesariamente necesitamos la misma cantidad de capas o los mismos tama\u00f1os de dimensiones ocultas en un modelo de sequence to sequence.\n", "Sin embargo, si hici\u00e9ramos algo como tener un n\u00famero diferente de capas, tendr\u00edamos que tomar decisiones sobre c\u00f3mo manejar esto.\n", "Por ejemplo, si nuestro encoder tiene 2 capas y nuestro decoder solo tiene 1, \u00bfc\u00f3mo se maneja esto? \u00bfPromediamos los dos vectores de contexto generados por el decoder?\n", "\u00bfPasamos ambos por una capa lineal? \u00bfSolo usamos el vector de contexto de la capa m\u00e1s alta? Etc.\n", "\n", "Nuestro m\u00e9todo \"forward\" toma la oraci\u00f3n fuente, la oraci\u00f3n objetivo y un ratio de teacher-forcing. El ratio de teacher-forcing se usa cuando entrenamos nuestro modelo.\n", "Al decodificar, en cada paso, predeciremos cu\u00e1l ser\u00e1 el pr\u00f3ximo token en la secuencia de destino de los tokens anteriores decodificados, $\\hat{y}_{t+1}=f(s_t^L)$.\n", "Con una probabilidad igual a la tasa de teacher forcing (`teacher_forcing_ratio`), utilizaremos el siguiente token real de la secuencia como entrada al decoder durante el siguiente paso.\n", "Sin embargo, con probabilidad `1 - Teacher_forcing_ratio`, usaremos el token que el modelo predijo como la pr\u00f3xima entrada al modelo, incluso si no coincide con el siguiente token real en la secuencia.\n", "\n", "Lo primero que hacemos en el m\u00e9todo `forward` es crear un tensor `outputs` que almacenar\u00e1 todas nuestras predicciones, $\\hat{Y}$.\n", "\n", "Luego alimentamos la oraci\u00f3n de entrada/fuente, `src`, en el encoder y recibimos los estados ocultos y de celda finales.\n", "\n", "La primera entrada al decoder es el token de inicio de secuencia (`<sos>`). Como nuestro tensor `trg` ya tiene el token `<sos>` agregado (desde cuando definimos el `init_token` en nuestro campo `TRG`)\n", "obtenemos nuestro $y_1$ cort\u00e1ndolo. Sabemos qu\u00e9 tan largas deben ser nuestras oraciones de destino (`max_len`), por lo que las repetimos muchas veces. El \u00faltimo token ingresado en el decoder es el **antes** del token `<eos>` - el `<eos>`\n", "el token nunca se ingresa en el decoder.\n", "\n", "Durante cada iteraci\u00f3n del ciclo, nosotros:\n", "- pasar la entrada, los estados de celda anteriores ocultos y anteriores ($y_t, s_{t-1}, c_{t-1}$) al decoder\n", "- recibir una predicci\u00f3n, el siguiente estado oculto y el siguiente estado de celda ($\\hat{y}_{t+1}, s_{t}, c_{t}$) del decoder\n", "- colocar nuestra predicci\u00f3n, $\\hat{y}_{t+1}$/`output` en nuestro tensor de predicciones, $\\hat{Y}$/`outputs`\n", "- decidir si vamos a \"fuerza de maestros\" o no\n", "     - si lo hacemos, la siguiente 'entrada' es el siguiente token de verdad fundamental en la secuencia, $y_{t+1}$/`trg[t]`\n", "     - si no lo hacemos, la siguiente `entrada` es el siguiente token predicho en la secuencia, $\\hat{y}_{t+1}$/`top1`, que obtenemos al hacer un `argmax` sobre el tensor de salida\n", "    \n", "Una vez que hemos hecho todas nuestras predicciones, devolvemos nuestro tensor lleno de predicciones, $\\hat{Y}$/`outputs`.\n", "\n", "**Nota**: nuestro ccilo decodificador comienza en 1, no en 0. Esto significa que el elemento 0 de nuestro tensor de `salidas` sigue siendo todo ceros. As\u00ed que nuestras `trg` y `outputs` se parecen a:\n", "\n", "$$\\begin{alinear*}\n", "\\text{trg} = [<sos>, &y_1, y_2, y_3, <eos>]\\\\\n", "\\text{resultados} = [0, &\\hat{y}_1, \\hat{y}_2, \\hat{y}_3, <eos>]\n", "\\end{align*}$$\n", "\n", "Posteriormente cuando calculamos la p\u00e9rdida, cortamos el primer elemento de cada tensor para obtener:\n", "\n", "$$\\begin{alinear*}\n", "\\text{trg} = [&y_1, y_2, y_3, <eos>]\\\\\n", "\\text{salidas} = [&\\hat{y}_1, \\hat{y}_2, \\hat{y}_3, <eos>]\n", "\\end{align*}$$"]}, {"cell_type": "code", "execution_count": 19, "id": "7f50e76b", "metadata": {"ExecuteTime": {"end_time": "2023-08-07T12:10:34.696215Z", "start_time": "2023-08-07T12:10:34.680165Z"}, "deletable": false, "nbgrader": {"cell_type": "code", "checksum": "a053dcb6ba362103fad11691a8c9cdfd", "grade": false, "grade_id": "cell-3cf708a546f162a5", "locked": false, "schema_version": 3, "solution": true, "task": false}, "id": "7f50e76b"}, "outputs": [], "source": ["class Seq2Seq(nn.Module):\n", "    def __init__(self, encoder, decoder, device):\n", "        super().__init__()\n", "\n", "        self.encoder = encoder\n", "        self.decoder = decoder\n", "        self.device = device\n", "\n", "        assert encoder.hid_dim == decoder.hid_dim, \\\n", "            \"Hidden dimensions of encoder and decoder must be equal!\"\n", "        assert encoder.n_layers == decoder.n_layers, \\\n", "            \"Encoder and decoder must have equal number of layers!\"\n", "\n", "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n", "\n", "        #src = [src len, batch size]\n", "        #trg = [trg len, batch size]\n", "        #teacher_forcing_ratio is probability to use teacher forcing\n", "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n", "\n", "        batch_size = trg.shape[1]\n", "        trg_len = trg.shape[0]\n", "        trg_vocab_size = self.decoder.output_dim\n", "\n", "        #tensor to store decoder outputs\n", "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n", "\n", "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n", "        hidden, cell = self.encoder(src)\n", "\n", "        #first input to the decoder is the <sos> tokens\n", "        input = trg[0,:]\n", "\n", "        for t in range(1, trg_len):\n", "\n", "            #insert input token embedding, previous hidden and previous cell states\n", "            #receive output tensor (predictions) and new hidden and cell states\n", "\n", "            # Aprox 1 linea para\n", "            # output, hidden, cell =\n", "            output, hidden, cell = self.decoder(input, hidden, cell)\n", "            # YOUR CODE HERE\n", "            # raise NotImplementedError()\n", "\n", "            #place predictions in a tensor holding predictions for each token\n", "            outputs[t] = output\n", "\n", "            #decide if we are going to use teacher forcing or not\n", "            teacher_force = random.random() < teacher_forcing_ratio\n", "\n", "            #get the highest predicted token from our predictions\n", "            top1 = output.argmax(1)\n", "\n", "            #if teacher forcing, use actual next token as next input\n", "            #if not, use predicted token\n", "            input = trg[t] if teacher_force else top1\n", "\n", "        return outputs\n"]}, {"cell_type": "markdown", "id": "e6723282", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "206f7ba1031ec9e31fc458fbe10ce7a4", "grade": false, "grade_id": "cell-8024abbda3c1898a", "locked": true, "schema_version": 3, "solution": false, "task": false}, "id": "e6723282"}, "source": ["### Training Seq2Seq Model\n", "\n", "Ahora que tenemos nuestro modelo implementado, podemos comenzar a entrenarlo.\n", "\n", "Primero, inicializaremos nuestro modelo. Como se mencion\u00f3 anteriormente, las dimensiones de entrada y salida est\u00e1n definidas por el tama\u00f1o del vocabulario.\n", "Las dimensiones de embedding y el dropout del encoder y el decoder pueden ser diferentes, pero el n\u00famero de capas y el tama\u00f1o de los estados ocultos/de celda deben ser los mismos.\n", "\n", "Luego definimos el encoder, el decoder y luego nuestro modelo Seq2Seq, que colocamos en el \"device\".\n", "\n", "\n", "El siguiente paso es inicializar los pesos de nuestro modelo. En el paper afirman que inicializan todos los pesos a partir de una distribuci\u00f3n uniforme entre -0,08 y +0,08, es decir, $\\mathcal{U}(-0,08, 0,08)$.\n", "\n", "Inicializamos los pesos en PyTorch creando una funci\u00f3n que \"aplicamos\" a nuestro modelo. Al usar `apply`, se llamar\u00e1 a la funci\u00f3n `init_weights` en cada m\u00f3dulo y subm\u00f3dulo dentro de nuestro modelo.\n", "Para cada m\u00f3dulo, recorremos todos los par\u00e1metros y los muestreamos desde una distribuci\u00f3n uniforme con `nn.init.uniform_`.\n", "\n", "\n", "Tambi\u00e9n definimos una funci\u00f3n que calcular\u00e1 el n\u00famero de par\u00e1metros entrenables en el modelo.\n", "\n", "\n", "Definimos nuestro optimizador, que usamos para actualizar nuestros par\u00e1metros en el ciclo de entrenamiento. Consulte [esta publicaci\u00f3n](http://ruder.io/optimizing-gradient-descent/)\n", "para obtener informaci\u00f3n sobre diferentes optimizadores. Aqu\u00ed usaremos a Adam\n", "\n", "A continuaci\u00f3n, definimos nuestra funci\u00f3n de p\u00e9rdida. La funci\u00f3n `CrossEntropyLoss` calcula tanto el log softmax como la log-likelihood negativo de nuestras predicciones.\n", "\n", "Nuestra funci\u00f3n de p\u00e9rdida calcula la p\u00e9rdida promedio por token, sin embargo, al pasar el \u00edndice del token `<pad>` como el argumento `ignore_index`, ignoramos la p\u00e9rdida siempre que el token de destino sea un token de relleno (padding)."]}, {"cell_type": "code", "execution_count": 20, "id": "ac8eac57", "metadata": {"ExecuteTime": {"end_time": "2023-08-07T12:10:41.167607Z", "start_time": "2023-08-07T12:10:35.024487Z"}, "id": "ac8eac57"}, "outputs": [], "source": ["INPUT_DIM = len(SRC.vocab)\n", "OUTPUT_DIM = len(TRG.vocab)\n", "ENC_EMB_DIM = 256\n", "DEC_EMB_DIM = 256\n", "HID_DIM = 512\n", "N_LAYERS = 2\n", "ENC_DROPOUT = 0.5\n", "DEC_DROPOUT = 0.5\n", "\n", "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n", "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n", "\n", "model = Seq2Seq(enc, dec, device).to(device)"]}, {"cell_type": "code", "execution_count": 21, "id": "04cb6773", "metadata": {"ExecuteTime": {"end_time": "2023-08-07T12:10:41.857202Z", "start_time": "2023-08-07T12:10:41.472920Z"}, "colab": {"base_uri": "https://localhost:8080/"}, "id": "04cb6773", "outputId": "e6db9bce-4ddf-479a-8992-7175ab285030"}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": ["Seq2Seq(\n", "  (encoder): Encoder(\n", "    (embedding): Embedding(7853, 256)\n", "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n", "    (dropout): Dropout(p=0.5, inplace=False)\n", "  )\n", "  (decoder): Decoder(\n", "    (embedding): Embedding(5893, 256)\n", "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n", "    (fc_out): Linear(in_features=512, out_features=5893, bias=True)\n", "    (dropout): Dropout(p=0.5, inplace=False)\n", "  )\n", ")"]}, "metadata": {}, "execution_count": 21}], "source": ["def init_weights(m):\n", "    for name, param in m.named_parameters():\n", "        nn.init.uniform_(param.data, -0.08, 0.08)\n", "\n", "model.apply(init_weights)"]}, {"cell_type": "code", "execution_count": 22, "id": "f7d5fffe", "metadata": {"ExecuteTime": {"end_time": "2023-08-07T12:10:42.370872Z", "start_time": "2023-08-07T12:10:42.339409Z"}, "colab": {"base_uri": "https://localhost:8080/"}, "id": "f7d5fffe", "outputId": "4d202eb0-72e7-416a-ca93-1e63b1655c71"}, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["El modelo tiene 13,898,501 parametros entrenables\n"]}], "source": ["def count_parameters(model):\n", "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n", "\n", "print(f'El modelo tiene {count_parameters(model):,} parametros entrenables')"]}, {"cell_type": "code", "execution_count": 23, "id": "c927da4e", "metadata": {"ExecuteTime": {"end_time": "2023-08-07T12:10:42.863269Z", "start_time": "2023-08-07T12:10:42.847270Z"}, "id": "c927da4e"}, "outputs": [], "source": ["optimizer = optim.Adam(model.parameters())\n"]}, {"cell_type": "code", "execution_count": 24, "id": "1655eb2b", "metadata": {"ExecuteTime": {"end_time": "2023-08-07T12:10:43.263371Z", "start_time": "2023-08-07T12:10:43.247371Z"}, "id": "1655eb2b"}, "outputs": [], "source": ["TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n", "\n", "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"]}, {"cell_type": "markdown", "id": "8aae5ea8", "metadata": {"id": "8aae5ea8"}, "source": ["A continuaci\u00f3n, definiremos nuestro ciclo de entrenamiento.\n", "\n", "Primero, configuraremos el modelo en \"modo de entrenamiento\" con `model.train()`. Esto activar\u00e1 el dropout (y batch normalization, que no estamos usando) y luego iterar\u00e1 a trav\u00e9s de nuestro iterador de datos.\n", "\n", "Como se indic\u00f3 anteriormente, nuestro ciclo decodificador comienza en 1, no en 0. Esto significa que el elemento 0 de nuestro tensor de \"salidas\" sigue siendo todo ceros. As\u00ed que nuestras `trg` y `outputs` se parecen a:\n", "\n", "$$\\begin{alinear*}\n", "\\text{trg} = [<sos>, &y_1, y_2, y_3, <eos>]\\\\\n", "\\text{resultados} = [0, &\\hat{y}_1, \\hat{y}_2, \\hat{y}_3, <eos>]\n", "\\end{align*}$$\n", "\n", "Aqu\u00ed, cuando calculamos la p\u00e9rdida, cortamos el primer elemento de cada tensor para obtener:\n", "\n", "$$\\begin{alinear*}\n", "\\text{trg} = [&y_1, y_2, y_3, <eos>]\\\\\n", "\\text{salidas} = [&\\hat{y}_1, \\hat{y}_2, \\hat{y}_3, <eos>]\n", "\\end{align*}$$\n", "\n", "En cada iteraci\u00f3n:\n", "- obtener las oraciones de origen y de destino del lote, $X$ y $Y$\n", "- poner a cero los gradientes calculados a partir del \u00faltimo lote\n", "- introduzca el origen y el destino en el modelo para obtener el resultado, $\\hat{Y}$\n", "- como la funci\u00f3n de p\u00e9rdida solo funciona en entradas 2d con objetivos 1d, necesitamos aplanar cada uno de ellos con `.view`\n", "     - cortamos la primera columna de los tensores de salida y destino como se mencion\u00f3 anteriormente\n", "- calcula los gradientes con `loss.backward()`\n", "- recorte los gradientes para evitar que exploten (un problema com\u00fan en RNN)\n", "- actualizar los par\u00e1metros de nuestro modelo haciendo un paso optimizador\n", "- sumar el valor de la p\u00e9rdida a un total acumulado\n", "\n", "Finalmente, devolvemos la p\u00e9rdida que se promedia en todos los batches."]}, {"cell_type": "code", "execution_count": 25, "id": "7a2c0aff", "metadata": {"ExecuteTime": {"end_time": "2023-08-07T12:10:43.863678Z", "start_time": "2023-08-07T12:10:43.847678Z"}, "deletable": false, "nbgrader": {"cell_type": "code", "checksum": "6ae8296e47d370619304a765919b81b2", "grade": false, "grade_id": "cell-5e78bda9de1a9bb9", "locked": false, "schema_version": 3, "solution": true, "task": false}, "id": "7a2c0aff"}, "outputs": [], "source": ["def train(model, iterator, optimizer, criterion, clip):\n", "\n", "    model.train()\n", "\n", "    epoch_loss = 0\n", "\n", "    for i, batch in enumerate(iterator):\n", "\n", "        src = batch.src\n", "        trg = batch.trg\n", "\n", "        # Aprox 1 linea para\n", "        # optimizer.zero...\n", "        optimizer.zero_grad()\n", "        # YOUR CODE HERE\n", "        # raise NotImplementedError()\n", "\n", "        output = model(src, trg)\n", "\n", "        #trg = [trg len, batch size]\n", "        #output = [trg len, batch size, output dim]\n", "\n", "        output_dim = output.shape[-1]\n", "\n", "        output = output[1:].view(-1, output_dim)\n", "        trg = trg[1:].view(-1)\n", "\n", "        #trg = [(trg len - 1) * batch size]\n", "        #output = [(trg len - 1) * batch size, output dim]\n", "\n", "        # Aprox 1 linea para\n", "        # loss =\n", "        loss = criterion(output, trg)\n", "        # YOUR CODE HERE\n", "        # raise NotImplementedError()\n", "\n", "        loss.backward()\n", "\n", "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n", "\n", "        optimizer.step()\n", "\n", "        epoch_loss += loss.item()\n", "\n", "    return epoch_loss / len(iterator)"]}, {"cell_type": "markdown", "id": "8efc0050", "metadata": {"id": "8efc0050"}, "source": ["Nuestro ciclo de evaluaci\u00f3n es similar a nuestro ciclo de entrenamiento, sin embargo, como no estamos actualizando ning\u00fan par\u00e1metro, no necesitamos pasar un optimizador o un valor de clip.\n", "\n", "Debemos recordar poner el modelo en modo de evaluaci\u00f3n con `model.eval()`. Esto desactivar\u00e1 el dropout (y la batch normalization, si se usa).\n", "\n", "Usamos el bloque `with torch.no_grad()` para garantizar que no se calculen gradientes dentro del bloque. Esto reduce el consumo de memoria y acelera el proceso.\n", "\n", "El ciclo de iteraci\u00f3n es similar (sin las actualizaciones de par\u00e1metros); sin embargo, debemos asegurarnos de desactivar el forzado del maestro para la evaluaci\u00f3n. }\n", "Esto har\u00e1 que el modelo solo use sus propias predicciones para hacer m\u00e1s predicciones dentro de una oraci\u00f3n, lo que refleja c\u00f3mo se usar\u00eda en la implementaci\u00f3n."]}, {"cell_type": "code", "execution_count": 26, "id": "75e62dbc", "metadata": {"ExecuteTime": {"end_time": "2023-08-07T12:10:44.241992Z", "start_time": "2023-08-07T12:10:44.225998Z"}, "id": "75e62dbc"}, "outputs": [], "source": ["def evaluate(model, iterator, criterion):\n", "\n", "    model.eval()\n", "\n", "    epoch_loss = 0\n", "\n", "    with torch.no_grad():\n", "\n", "        for i, batch in enumerate(iterator):\n", "\n", "            src = batch.src\n", "            trg = batch.trg\n", "\n", "            output = model(src, trg, 0) #turn off teacher forcing\n", "\n", "            #trg = [trg len, batch size]\n", "            #output = [trg len, batch size, output dim]\n", "\n", "            output_dim = output.shape[-1]\n", "\n", "            output = output[1:].view(-1, output_dim)\n", "            trg = trg[1:].view(-1)\n", "\n", "            #trg = [(trg len - 1) * batch size]\n", "            #output = [(trg len - 1) * batch size, output dim]\n", "\n", "            loss = criterion(output, trg)\n", "\n", "            epoch_loss += loss.item()\n", "\n", "    return epoch_loss / len(iterator)"]}, {"cell_type": "markdown", "id": "d4e696c7", "metadata": {"id": "d4e696c7"}, "source": ["A continuaci\u00f3n, crearemos una funci\u00f3n que usaremos para decirnos cu\u00e1nto tarda una \u00e9poca.\n"]}, {"cell_type": "code", "execution_count": 27, "id": "68230473", "metadata": {"ExecuteTime": {"end_time": "2023-08-07T12:10:44.738487Z", "start_time": "2023-08-07T12:10:44.726276Z"}, "id": "68230473"}, "outputs": [], "source": ["def epoch_time(start_time, end_time):\n", "    elapsed_time = end_time - start_time\n", "    elapsed_mins = int(elapsed_time / 60)\n", "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n", "    return elapsed_mins, elapsed_secs"]}, {"cell_type": "markdown", "id": "3fe81d58", "metadata": {"id": "3fe81d58"}, "source": ["Ahora s\u00ed, \u00a1empecemos a entrenar a nuestro modelo!\n", "\n", "En cada \u00e9poca, comprobaremos si nuestro modelo ha logrado la mejor p\u00e9rdida de validaci\u00f3n hasta el momento. Si es as\u00ed, actualizaremos nuestra mejor p\u00e9rdida de validaci\u00f3n y guardaremos los par\u00e1metros de nuestro modelo\n", "(llamado `state_dict` en PyTorch). Luego, cuando lleguemos a probar nuestro modelo, usaremos los par\u00e1metros guardados para lograr la mejor p\u00e9rdida de validaci\u00f3n.\n", "\n", "Estaremos mostrando tanto la p\u00e9rdida como la perplejidad en cada \u00e9poca. Es m\u00e1s f\u00e1cil ver un cambio en la perplejidad que un cambio en la p\u00e9rdida ya que los n\u00fameros son mucho mayores.\n", "\n", "Ademas, cargaremos los par\u00e1metros (`state_dict`) que dieron a nuestro modelo la mejor p\u00e9rdida de validaci\u00f3n y ejecutaremos el modelo en el conjunto de prueba."]}, {"cell_type": "code", "execution_count": 29, "id": "7b0215ac", "metadata": {"ExecuteTime": {"end_time": "2023-08-07T12:18:56.768139Z", "start_time": "2023-08-07T12:10:46.646109Z"}, "deletable": false, "nbgrader": {"cell_type": "code", "checksum": "3abe75f3dec72dbef634b76eecb3cb54", "grade": false, "grade_id": "cell-c2a7405dde118a6e", "locked": false, "schema_version": 3, "solution": true, "task": false}, "colab": {"base_uri": "https://localhost:8080/"}, "id": "7b0215ac", "outputId": "0eea8f25-12dc-487a-a381-f0c81f76ccbc"}, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["Epoch: 01 | Time: 18m 13s\n", "\tTrain Loss: 4.895 | Train PPL: 133.642\n", "\t Val. Loss: 4.877 |  Val. PPL: 131.257\n", "Epoch: 02 | Time: 18m 32s\n", "\tTrain Loss: 4.459 | Train PPL:  86.375\n", "\t Val. Loss: 4.759 |  Val. PPL: 116.587\n", "Epoch: 03 | Time: 18m 33s\n", "\tTrain Loss: 4.215 | Train PPL:  67.690\n", "\t Val. Loss: 4.666 |  Val. PPL: 106.267\n", "Epoch: 04 | Time: 18m 15s\n", "\tTrain Loss: 4.028 | Train PPL:  56.139\n", "\t Val. Loss: 4.482 |  Val. PPL:  88.402\n", "Epoch: 05 | Time: 18m 9s\n", "\tTrain Loss: 3.900 | Train PPL:  49.389\n", "\t Val. Loss: 4.514 |  Val. PPL:  91.251\n", "Epoch: 06 | Time: 18m 6s\n", "\tTrain Loss: 3.790 | Train PPL:  44.278\n", "\t Val. Loss: 4.560 |  Val. PPL:  95.568\n", "Epoch: 07 | Time: 17m 59s\n", "\tTrain Loss: 3.691 | Train PPL:  40.088\n", "\t Val. Loss: 4.345 |  Val. PPL:  77.125\n"]}], "source": ["# para que pueda definir\n", "N_EPOCHS = 7\n", "CLIP = 1\n", "# YOUR CODE HERE\n", "#raise NotImplementedError()\n", "\n", "best_valid_loss = float('inf')\n", "\n", "for epoch in range(N_EPOCHS):\n", "\n", "    start_time = time.time()\n", "\n", "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n", "    valid_loss = evaluate(model, valid_iterator, criterion)\n", "\n", "    end_time = time.time()\n", "\n", "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n", "\n", "    if valid_loss < best_valid_loss:\n", "        best_valid_loss = valid_loss\n", "        torch.save(model.state_dict(), 'tut1-model.pt')\n", "\n", "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n", "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n", "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"]}, {"cell_type": "code", "execution_count": 33, "id": "71971818", "metadata": {"ExecuteTime": {"end_time": "2023-08-07T22:26:39.976008Z", "start_time": "2023-08-07T22:26:39.653650Z"}, "deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "6f8a150031eecd843b79d77d31d64804", "grade": true, "grade_id": "cell-b70f37945f5a9981", "locked": true, "points": 50, "schema_version": 3, "solution": false, "task": false}, "id": "71971818", "colab": {"base_uri": "https://localhost:8080/", "height": 128}, "outputId": "ed5a0cd6-ab60-4167-c83f-e660c2994127"}, "outputs": [{"output_type": "display_data", "data": {"text/plain": ["<IPython.core.display.HTML object>"], "text/html": ["\n", "        <div class=\"alert alert-box alert-success\">\n", "        <h1> <!--{id:\"CORRECTMARK\", marks:\"25\"}--> \n", "         \u2713 [25 marks] \n", "         </h1> </div>"]}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["<IPython.core.display.HTML object>"], "text/html": ["\n", "        <div class=\"alert alert-box alert-success\">\n", "        <h1> <!--{id:\"CORRECTMARK\", marks:\"25\"}--> \n", "         \u2713 [25 marks] \n", "         </h1> </div>"]}, "metadata": {}}], "source": ["# Se valuara que el loss de training sea menor a 4 y el de validacion a 4.5\n", "\n", "with tick.marks(25):\n", "    assert compare_numbers(new_representation(train_loss), \"3c3d\", '0x1.0000000000000p+2')\n", "\n", "with tick.marks(25):\n", "    assert compare_numbers(new_representation(valid_loss), \"3c3d\", '0x1.2000000000000p+2')"]}, {"cell_type": "code", "execution_count": 34, "id": "fe2c01e5", "metadata": {"ExecuteTime": {"end_time": "2023-08-07T12:25:14.650820Z", "start_time": "2023-08-07T12:25:12.622015Z"}, "id": "fe2c01e5", "colab": {"base_uri": "https://localhost:8080/"}, "outputId": "e8ccf22a-ec4b-4f1e-8fed-98e147ff78bc"}, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["| Test Loss: 4.365 | Test PPL:  78.621 |\n"]}], "source": ["model.load_state_dict(torch.load('tut1-model.pt'))\n", "\n", "test_loss = evaluate(model, test_iterator, criterion)\n", "\n", "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"]}, {"cell_type": "code", "execution_count": 35, "id": "a77cbb7c", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "58fd9a560ef4d1a143e87ce331286237", "grade": true, "grade_id": "cell-e94ae9af3a4c26ff", "locked": true, "points": 0, "schema_version": 3, "solution": false, "task": false}, "id": "a77cbb7c", "colab": {"base_uri": "https://localhost:8080/", "height": 109}, "outputId": "56531574-aa0a-4f5b-f092-e49e7f404cd9"}, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["\n", "La fraccion de abajo muestra su rendimiento basado en las partes visibles de este laboratorio\n"]}, {"output_type": "display_data", "data": {"text/plain": ["<IPython.core.display.HTML object>"], "text/html": ["<!--{id:\"TOTALMARK\",marks:\"50\", available:\"50\"}  -->\n", "        \n", "        <h1> 50 / 50 marks (100.0%) </h1>\n", "        "]}, "metadata": {}}], "source": ["\n", "print()\n", "print(\"La fraccion de abajo muestra su rendimiento basado en las partes visibles de este laboratorio\")\n", "tick.summarise_marks() #"]}], "metadata": {"hide_input": false, "kernelspec": {"display_name": "venv", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.23"}, "toc": {"base_numbering": 1, "nav_menu": {}, "number_sections": true, "sideBar": true, "skip_h1_title": false, "title_cell": "Table of Contents", "title_sidebar": "Contents", "toc_cell": false, "toc_position": {}, "toc_section_display": true, "toc_window_display": false}, "varInspector": {"cols": {"lenName": 16, "lenType": 16, "lenVar": 40}, "kernels_config": {"python": {"delete_cmd_postfix": "", "delete_cmd_prefix": "del ", "library": "var_list.py", "varRefreshCmd": "print(var_dic_list())"}, "r": {"delete_cmd_postfix": ") ", "delete_cmd_prefix": "rm(", "library": "var_list.r", "varRefreshCmd": "cat(var_dic_list()) "}}, "types_to_exclude": ["module", "function", "builtin_function_or_method", "instance", "_Feature"], "window_display": false}, "colab": {"provenance": []}, "widgets": {"application/vnd.jupyter.widget-state+json": {"822b06c6edad46aa97086a8eb24b529f": {"model_module": "@jupyter-widgets/controls", "model_name": "HBoxModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_46a814a3dff24304a9827996b1b17ba6", "IPY_MODEL_73ad7418592b4d4dafa7cff6e0479e48", "IPY_MODEL_a2bd24080f694dab97584227a33bd15d"], "layout": "IPY_MODEL_ef06f016e6224f6bb34242a569b978f8"}}, "46a814a3dff24304a9827996b1b17ba6": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_85a3ec21e2dc4a7bb7e6f2b624db4f35", "placeholder": "\u200b", "style": "IPY_MODEL_b06c521beeb0484ab26f1f6777e3fd39", "value": "README.md:\u2007"}}, "73ad7418592b4d4dafa7cff6e0479e48": {"model_module": "@jupyter-widgets/controls", "model_name": "FloatProgressModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_b2ccc571b1ae4a22aa2f07d91ffc1982", "max": 1, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_ba311895ae2f441888dcb34b500d1d4f", "value": 1}}, "a2bd24080f694dab97584227a33bd15d": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_311c4805d59341c89277f1ea5e8030a4", "placeholder": "\u200b", "style": "IPY_MODEL_71c8413d3f2b4546ab2ca5e2ab17fc84", "value": "\u20075.58k/?\u2007[00:00&lt;00:00,\u2007379kB/s]"}}, "ef06f016e6224f6bb34242a569b978f8": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "85a3ec21e2dc4a7bb7e6f2b624db4f35": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "b06c521beeb0484ab26f1f6777e3fd39": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "b2ccc571b1ae4a22aa2f07d91ffc1982": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": "20px"}}, "ba311895ae2f441888dcb34b500d1d4f": {"model_module": "@jupyter-widgets/controls", "model_name": "ProgressStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "311c4805d59341c89277f1ea5e8030a4": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "71c8413d3f2b4546ab2ca5e2ab17fc84": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "e2a73548943447fa8b1c7d6a2cc9f0dd": {"model_module": "@jupyter-widgets/controls", "model_name": "HBoxModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_5673bc25e0ab408b9a6ceb9e3931bdde", "IPY_MODEL_b9ba3986526f404db2e9a6edc7947367", "IPY_MODEL_dfa4a69b1a5a4c01939e351937dcae80"], "layout": "IPY_MODEL_6ae8a6a99d9441829d1b9a196d772ee6"}}, "5673bc25e0ab408b9a6ceb9e3931bdde": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_423204beaa984aa6ad0a6396af1319f7", "placeholder": "\u200b", "style": "IPY_MODEL_5e38add836454ae99d33098d85712b25", "value": "train-00000-of-00001.parquet:\u2007100%"}}, "b9ba3986526f404db2e9a6edc7947367": {"model_module": "@jupyter-widgets/controls", "model_name": "FloatProgressModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_e2d48af9b6f54847ad89f8f5d54f2077", "max": 2068237, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_46bb8aff6cae4a3094db4f92401ed5ec", "value": 2068237}}, "dfa4a69b1a5a4c01939e351937dcae80": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_c22d1ec5f2684b05aaac3478b8497d31", "placeholder": "\u200b", "style": "IPY_MODEL_236eb25c1db74e7782c56d95f41efdf8", "value": "\u20072.07M/2.07M\u2007[00:00&lt;00:00,\u200710.5MB/s]"}}, "6ae8a6a99d9441829d1b9a196d772ee6": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "423204beaa984aa6ad0a6396af1319f7": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "5e38add836454ae99d33098d85712b25": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "e2d48af9b6f54847ad89f8f5d54f2077": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "46bb8aff6cae4a3094db4f92401ed5ec": {"model_module": "@jupyter-widgets/controls", "model_name": "ProgressStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "c22d1ec5f2684b05aaac3478b8497d31": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "236eb25c1db74e7782c56d95f41efdf8": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "05e9876fc12f4c86adaf0b16ea960cf1": {"model_module": "@jupyter-widgets/controls", "model_name": "HBoxModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_1487fc0512ff4a758bc32d6e03ebdb19", "IPY_MODEL_b5b55d83108d40c79f3913e246710082", "IPY_MODEL_be59aa2898634dea833140dd1ae121e5"], "layout": "IPY_MODEL_f5189231ce824e00a1e1db51d76d83aa"}}, "1487fc0512ff4a758bc32d6e03ebdb19": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_cbaf1997f70c495f92e6aa7e7d41e73e", "placeholder": "\u200b", "style": "IPY_MODEL_cd07ec76077c491ab156b347424a80e5", "value": "test-00000-of-00001.parquet:\u2007100%"}}, "b5b55d83108d40c79f3913e246710082": {"model_module": "@jupyter-widgets/controls", "model_name": "FloatProgressModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_d28b092b5205459d8db0e563eecb103e", "max": 1112032, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_1b369b2b232d4253949a38c3ebda2a0f", "value": 1112032}}, "be59aa2898634dea833140dd1ae121e5": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_488f109bd9944239ab84027134f83389", "placeholder": "\u200b", "style": "IPY_MODEL_58ffb4303adc4db4a9673c6313af9789", "value": "\u20071.11M/1.11M\u2007[00:00&lt;00:00,\u200725.7MB/s]"}}, "f5189231ce824e00a1e1db51d76d83aa": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "cbaf1997f70c495f92e6aa7e7d41e73e": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "cd07ec76077c491ab156b347424a80e5": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "d28b092b5205459d8db0e563eecb103e": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "1b369b2b232d4253949a38c3ebda2a0f": {"model_module": "@jupyter-widgets/controls", "model_name": "ProgressStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "488f109bd9944239ab84027134f83389": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "58ffb4303adc4db4a9673c6313af9789": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "258542da13d74058954ff9113cc68e4b": {"model_module": "@jupyter-widgets/controls", "model_name": "HBoxModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_1585059b6e3d4ca98bd783bfd753d81c", "IPY_MODEL_2b81483103b14cfd96670f175373a23e", "IPY_MODEL_dc4b24c50a3f41cbb4ba3ac5854e88db"], "layout": "IPY_MODEL_8ddf56d0aaf642679e466a0521560101"}}, "1585059b6e3d4ca98bd783bfd753d81c": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_36f43eeef4664352aaa20bdcf9ddb8dd", "placeholder": "\u200b", "style": "IPY_MODEL_0e1693cf99454998a82a4c3de10be455", "value": "Generating\u2007train\u2007split:\u2007100%"}}, "2b81483103b14cfd96670f175373a23e": {"model_module": "@jupyter-widgets/controls", "model_name": "FloatProgressModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_533820de87da4f81a9bf050b9ff9c7df", "max": 31962, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_baff436ea7ea4d4f9fe1d8eb03c907ab", "value": 31962}}, "dc4b24c50a3f41cbb4ba3ac5854e88db": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_16b4008c6a98458b9a8e86b4b28ce07e", "placeholder": "\u200b", "style": "IPY_MODEL_1b54241282a04710af20970b7511156d", "value": "\u200731962/31962\u2007[00:00&lt;00:00,\u2007370706.84\u2007examples/s]"}}, "8ddf56d0aaf642679e466a0521560101": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "36f43eeef4664352aaa20bdcf9ddb8dd": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "0e1693cf99454998a82a4c3de10be455": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "533820de87da4f81a9bf050b9ff9c7df": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "baff436ea7ea4d4f9fe1d8eb03c907ab": {"model_module": "@jupyter-widgets/controls", "model_name": "ProgressStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "16b4008c6a98458b9a8e86b4b28ce07e": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "1b54241282a04710af20970b7511156d": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "84582b5aa04e4b079f829fc475827b97": {"model_module": "@jupyter-widgets/controls", "model_name": "HBoxModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_e14e440001af4ce5b8509d643eb05e2e", "IPY_MODEL_b6040dbaef1a447794f669966e220084", "IPY_MODEL_a30db461f1ad4977b9f21b6cae37cd06"], "layout": "IPY_MODEL_eb20fc64ccde43d4b3ba0037f327c12c"}}, "e14e440001af4ce5b8509d643eb05e2e": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_6729b757046540ec815aaf8f74ea098a", "placeholder": "\u200b", "style": "IPY_MODEL_7f5b77c9d0854c5fa186b983ab7bf1ca", "value": "Generating\u2007test\u2007split:\u2007100%"}}, "b6040dbaef1a447794f669966e220084": {"model_module": "@jupyter-widgets/controls", "model_name": "FloatProgressModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_4526d14c2305464197efcff8754665de", "max": 17197, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_c52a5453a14b427b92cc173865e97d9d", "value": 17197}}, "a30db461f1ad4977b9f21b6cae37cd06": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_a5a668de9c3b4787a84f9ed8194d439a", "placeholder": "\u200b", "style": "IPY_MODEL_42ec03202f3c44158a9d8927236c4694", "value": "\u200717197/17197\u2007[00:00&lt;00:00,\u2007338259.52\u2007examples/s]"}}, "eb20fc64ccde43d4b3ba0037f327c12c": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "6729b757046540ec815aaf8f74ea098a": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "7f5b77c9d0854c5fa186b983ab7bf1ca": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "4526d14c2305464197efcff8754665de": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "c52a5453a14b427b92cc173865e97d9d": {"model_module": "@jupyter-widgets/controls", "model_name": "ProgressStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "a5a668de9c3b4787a84f9ed8194d439a": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "42ec03202f3c44158a9d8927236c4694": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}}}}, "nbformat": 4, "nbformat_minor": 5}